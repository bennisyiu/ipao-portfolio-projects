{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8806865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyalex in ./myenv/lib/python3.12/site-packages (0.18)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from pyalex) (2.32.3)\n",
      "Requirement already satisfied: urllib3 in ./myenv/lib/python3.12/site-packages (from pyalex) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->pyalex) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->pyalex) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->pyalex) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyalex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0e482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyalex\n",
    "from pyalex import Works\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os  # To check if CSV exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e005456",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test run\n",
    "\n",
    "# Optional: Set your email for the OpenAlex polite pool\n",
    "pyalex.config.email = \"bennis.yiu@connect.polyu.hk\"\n",
    "\n",
    "# List of DOIs to query\n",
    "dois_to_query = [\n",
    "    \"10.1177/10963480241229235\",\n",
    "    \"10.1002/adfm.202413884\",\n",
    "    \"10.1109/TNNLS.2023.3336563\",\n",
    "    \"10.1016/j.esci.2024.100281\",\n",
    "    \"10.1109/TEVC.2023.3278132\"\n",
    "]\n",
    "\n",
    "def get_openalex_data_for_dois(doi_list):\n",
    "    \"\"\"\n",
    "    Retrieves publication data from OpenAlex for a list of DOIs.\n",
    "\n",
    "    Args:\n",
    "        doi_list: A list of DOI strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains\n",
    "        information for a successfully found DOI. Returns an empty\n",
    "        list if no data is found or an error occurs.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    print(f\"Querying OpenAlex for {len(doi_list)} DOIs...\")\n",
    "\n",
    "    for doi in doi_list:\n",
    "        print(f\"\\n--- Processing DOI: {doi} ---\")\n",
    "        try:\n",
    "            # Construct the full DOI URL for querying\n",
    "            # pyalex often works better with the full URL format\n",
    "            full_doi_url = f\"https://doi.org/{doi.lower()}\" # Lowercase is good practice\n",
    "\n",
    "            # Query OpenAlex Works endpoint by DOI\n",
    "            work = Works()[full_doi_url] # Using dict-like access requires the full URL\n",
    "\n",
    "            if not work:\n",
    "                print(f\"  DOI not found in OpenAlex: {doi}\")\n",
    "                continue\n",
    "\n",
    "            # --- Extract desired information ---\n",
    "            # Basic Info\n",
    "            title = work.get('title', 'N/A')\n",
    "            pub_year = work.get('publication_year', 'N/A')\n",
    "            openalex_id = work.get('id', 'N/A')\n",
    "            cited_by_count = work.get('cited_by_count', 0)\n",
    "            journal_name = work.get('host_venue', {}).get('display_name', 'N/A')\n",
    "            journal_issn = work.get('host_venue', {}).get('issn_l', 'N/A') # Linking ISSN\n",
    "\n",
    "            # Authors and Affiliations\n",
    "            authors_info = []\n",
    "            if work.get('authorships'):\n",
    "                for authorship in work['authorships']:\n",
    "                    author_name = authorship.get('author', {}).get('display_name', 'N/A')\n",
    "                    author_orcid = authorship.get('author', {}).get('orcid') # Might be None\n",
    "                    institutions = authorship.get('institutions', [])\n",
    "                    institution_names = [inst.get('display_name', 'N/A') for inst in institutions]\n",
    "                    authors_info.append({\n",
    "                        \"name\": author_name,\n",
    "                        \"orcid\": author_orcid,\n",
    "                        \"institutions\": institution_names\n",
    "                    })\n",
    "\n",
    "            # Concepts (Topics/Subjects) - Let's take the top 3\n",
    "            concepts_info = []\n",
    "            if work.get('concepts'):\n",
    "                # Sort concepts by score (descending) and take top 3\n",
    "                sorted_concepts = sorted(work['concepts'], key=lambda x: x.get('score', 0), reverse=True)\n",
    "                for concept in sorted_concepts[:3]:\n",
    "                    concepts_info.append({\n",
    "                        \"name\": concept.get('display_name', 'N/A'),\n",
    "                        \"level\": concept.get('level', 'N/A'),\n",
    "                        \"score\": concept.get('score', 'N/A')\n",
    "                    })\n",
    "\n",
    "            # Store extracted data\n",
    "            extracted_data = {\n",
    "                \"doi\": doi,\n",
    "                \"openalex_id\": openalex_id,\n",
    "                \"title\": title,\n",
    "                \"publication_year\": pub_year,\n",
    "                \"cited_by_count\": cited_by_count,\n",
    "                \"journal\": journal_name,\n",
    "                \"journal_issn_l\": journal_issn,\n",
    "                \"authors\": authors_info,\n",
    "                \"concepts\": concepts_info,\n",
    "                # Add more fields here if needed by exploring the 'work' object\n",
    "                # e.g., 'type', 'abstract_inverted_index', 'referenced_works', 'related_works'\n",
    "            }\n",
    "            results.append(extracted_data)\n",
    "\n",
    "            # --- Print some key retrieved info ---\n",
    "            print(f\"  Title: {title}\")\n",
    "            print(f\"  Journal: {journal_name} ({pub_year})\")\n",
    "            print(f\"  Authors: {', '.join([a['name'] for a in authors_info])}\")\n",
    "            print(f\"  Top Concepts: {', '.join([c['name'] for c in concepts_info])}\")\n",
    "            print(f\"  Cited By: {cited_by_count}\")\n",
    "            print(f\"  OpenAlex ID: {openalex_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing DOI {doi}: {e}\")\n",
    "            # This could be a network error, API error, or the DOI truly not existing\n",
    "\n",
    "    print(\"\\n--- Finished Querying ---\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62327418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the function with the list of DOIs\n",
    "    retrieved_data = get_openalex_data_for_dois(dois_to_query)\n",
    "\n",
    "    # You can now work with the 'retrieved_data' list, which contains\n",
    "    # dictionaries of information for each successfully processed DOI.\n",
    "    print(f\"\\nSuccessfully retrieved data for {len(retrieved_data)} out of {len(dois_to_query)} DOIs.\")\n",
    "\n",
    "    # Example: Print the title of the first result if available\n",
    "    if retrieved_data:\n",
    "        print(f\"\\nExample - Title of first result: {retrieved_data[0].get('title', 'N/A')}\")\n",
    "\n",
    "    # Example: Print author names and institutions for the second result if available\n",
    "    if len(retrieved_data) > 1:\n",
    "        print(\"\\nExample - Authors/Institutions of second result:\")\n",
    "        for author_info in retrieved_data[1].get('authors', []):\n",
    "            print(f\"  - {author_info['name']} ({', '.join(author_info['institutions'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c062ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_data_df = pd.DataFrame(retrieved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076fed43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "openalex_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publication_year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cited_by_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "journal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "journal_issn_l",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "concepts",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a1a65b10-f5db-4e10-804b-0922ab7b5915",
       "rows": [
        [
         "0",
         "10.1177/10963480241229235",
         "https://openalex.org/W4391822953",
         "Artificial Intelligence in Hospitality and Tourism: Insights From Industry Practices, Research Literature, and Expert Opinions",
         "2024",
         "30",
         "N/A",
         "N/A",
         "[{'name': 'Hyunsu Kim', 'orcid': 'https://orcid.org/0000-0003-0103-9313', 'institutions': ['California State University, Fullerton']}, {'name': 'Kevin Kam Fung So', 'orcid': 'https://orcid.org/0000-0002-4846-7481', 'institutions': ['Oklahoma State University', 'Kyung Hee University']}, {'name': 'Seunghun Shin', 'orcid': 'https://orcid.org/0000-0001-7022-6732', 'institutions': ['Hong Kong Polytechnic University']}, {'name': 'Jing Li', 'orcid': 'https://orcid.org/0000-0003-3621-0838', 'institutions': ['Texas Tech University']}]",
         "[{'name': 'Hospitality', 'level': 3, 'score': 0.8348868}, {'name': 'Tourism', 'level': 2, 'score': 0.8121532}, {'name': 'Hospitality industry', 'level': 3, 'score': 0.68744355}]"
        ],
        [
         "1",
         "10.1002/adfm.202413884",
         "https://openalex.org/W4403192979",
         "Hierarchical Engineering on Built‐In Electric Field of Bimetallic Zeolitic Imidazolate Derivatives Towards Amplified Dielectric Loss",
         "2024",
         "26",
         "N/A",
         "N/A",
         "[{'name': 'Shijie Zhang', 'orcid': 'https://orcid.org/0000-0002-9269-5406', 'institutions': ['Henan University of Technology']}, {'name': 'Jiajun Zheng', 'orcid': 'https://orcid.org/0000-0001-7292-2678', 'institutions': ['Henan University of Technology']}, {'name': 'Di Lan', 'orcid': 'https://orcid.org/0000-0001-8570-4450', 'institutions': ['Hubei University of Automotive Technology']}, {'name': 'Zhenguo Gao', 'orcid': 'https://orcid.org/0000-0002-5953-1514', 'institutions': ['Qingdao University', 'Hong Kong Polytechnic University']}, {'name': 'Xiaowei Liang', 'orcid': None, 'institutions': ['Henan University of Technology']}, {'name': 'Qingfeng Tian', 'orcid': None, 'institutions': ['Henan University of Technology']}, {'name': 'Zhiwei Zhao', 'orcid': None, 'institutions': ['Henan University of Technology']}, {'name': 'Guanglei Wu', 'orcid': 'https://orcid.org/0000-0001-6145-3415', 'institutions': ['Qingdao University']}]",
         "[{'name': 'Materials science', 'level': 0, 'score': 0.8805107}, {'name': 'Bimetallic strip', 'level': 3, 'score': 0.86678255}, {'name': 'Zeolitic imidazolate framework', 'level': 4, 'score': 0.79701996}]"
        ],
        [
         "2",
         "10.1109/TNNLS.2023.3336563",
         "https://openalex.org/W4391130239",
         "Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images",
         "2024",
         "33",
         "N/A",
         "N/A",
         "[{'name': 'Ye Liu', 'orcid': 'https://orcid.org/0000-0001-9597-0525', 'institutions': ['Wuhan University', 'Hong Kong Polytechnic University']}, {'name': 'Huifang Li', 'orcid': 'https://orcid.org/0000-0003-4626-7416', 'institutions': ['Wuhan University']}, {'name': 'Chao Hu', 'orcid': 'https://orcid.org/0000-0001-6183-9051', 'institutions': ['Wuhan University']}, {'name': 'Shuang Luo', 'orcid': 'https://orcid.org/0000-0003-3832-0475', 'institutions': ['Jiangsu Changjiang Electronics Technology (China)']}, {'name': 'Yan Luo', 'orcid': 'https://orcid.org/0000-0002-9533-6070', 'institutions': ['Hong Kong Polytechnic University']}, {'name': 'Chang Wen Chen', 'orcid': 'https://orcid.org/0000-0002-6720-234X', 'institutions': ['Hong Kong Polytechnic University', 'Peng Cheng Laboratory']}]",
         "[{'name': 'Computer science', 'level': 0, 'score': 0.82638764}, {'name': 'Pyramid (geometry)', 'level': 2, 'score': 0.7803607}, {'name': 'Context (archaeology)', 'level': 2, 'score': 0.6926302}]"
        ],
        [
         "3",
         "10.1016/j.esci.2024.100281",
         "https://openalex.org/W4398247259",
         "Heterogeneous structure design for stable Li/Na metal batteries: Progress and prospects",
         "2024",
         "29",
         "N/A",
         "N/A",
         "[{'name': 'Hongyang Chen', 'orcid': 'https://orcid.org/0000-0002-7626-0162', 'institutions': ['Fujian Normal University']}, {'name': 'Junxiong Wu', 'orcid': None, 'institutions': ['Fujian Normal University']}, {'name': 'Manxian Li', 'orcid': None, 'institutions': ['Fujian Normal University']}, {'name': 'Jingyue Zhao', 'orcid': None, 'institutions': ['Fujian Normal University']}, {'name': 'Zulin Li', 'orcid': None, 'institutions': ['Fujian Normal University']}, {'name': 'Manxi Wang', 'orcid': 'https://orcid.org/0000-0001-9266-8611', 'institutions': ['Fujian Normal University']}, {'name': 'Xuan Li', 'orcid': 'https://orcid.org/0000-0001-5066-0523', 'institutions': ['Fujian Normal University']}, {'name': 'Chuanping Li', 'orcid': 'https://orcid.org/0000-0002-4985-3210', 'institutions': ['Fujian Normal University']}, {'name': 'Xiaochuan Chen', 'orcid': 'https://orcid.org/0000-0002-5720-7657', 'institutions': ['Fujian Normal University']}, {'name': 'Xiaoyan Li', 'orcid': 'https://orcid.org/0000-0001-7362-7128', 'institutions': ['Fujian Normal University']}, {'name': 'Yiu‐Wing Mai', 'orcid': 'https://orcid.org/0000-0002-9141-2793', 'institutions': ['Hong Kong Polytechnic University']}, {'name': 'Yuming Chen', 'orcid': 'https://orcid.org/0000-0002-7180-9515', 'institutions': ['Fujian Normal University']}]",
         "[{'name': 'Battery (electricity)', 'level': 3, 'score': 0.6079239}, {'name': 'Rational design', 'level': 2, 'score': 0.6068746}, {'name': 'Deposition (geology)', 'level': 3, 'score': 0.58796227}]"
        ],
        [
         "4",
         "10.1109/TEVC.2023.3278132",
         "https://openalex.org/W4381983181",
         "Knowledge Learning for Evolutionary Computation",
         "2023",
         "27",
         "N/A",
         "N/A",
         "[{'name': 'Yi Jiang', 'orcid': 'https://orcid.org/0000-0003-1965-0372', 'institutions': ['South China University of Technology']}, {'name': 'Zhi‐Hui Zhan', 'orcid': 'https://orcid.org/0000-0003-0862-0514', 'institutions': ['South China University of Technology']}, {'name': 'Kay Chen Tan', 'orcid': 'https://orcid.org/0000-0002-6802-2463', 'institutions': ['Hong Kong Polytechnic University']}, {'name': 'Jun Zhang', 'orcid': 'https://orcid.org/0000-0001-7835-9871', 'institutions': ['Zhejiang Normal University', 'Hanyang University']}]",
         "[{'name': 'Computer science', 'level': 0, 'score': 0.737087}, {'name': 'Evolutionary computation', 'level': 2, 'score': 0.69463915}, {'name': 'Artificial intelligence', 'level': 1, 'score': 0.67390084}]"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>openalex_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_issn_l</th>\n",
       "      <th>authors</th>\n",
       "      <th>concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1177/10963480241229235</td>\n",
       "      <td>https://openalex.org/W4391822953</td>\n",
       "      <td>Artificial Intelligence in Hospitality and Tou...</td>\n",
       "      <td>2024</td>\n",
       "      <td>30</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[{'name': 'Hyunsu Kim', 'orcid': 'https://orci...</td>\n",
       "      <td>[{'name': 'Hospitality', 'level': 3, 'score': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002/adfm.202413884</td>\n",
       "      <td>https://openalex.org/W4403192979</td>\n",
       "      <td>Hierarchical Engineering on Built‐In Electric ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>26</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[{'name': 'Shijie Zhang', 'orcid': 'https://or...</td>\n",
       "      <td>[{'name': 'Materials science', 'level': 0, 'sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1109/TNNLS.2023.3336563</td>\n",
       "      <td>https://openalex.org/W4391130239</td>\n",
       "      <td>Learning to Aggregate Multi-Scale Context for ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>33</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[{'name': 'Ye Liu', 'orcid': 'https://orcid.or...</td>\n",
       "      <td>[{'name': 'Computer science', 'level': 0, 'sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.esci.2024.100281</td>\n",
       "      <td>https://openalex.org/W4398247259</td>\n",
       "      <td>Heterogeneous structure design for stable Li/N...</td>\n",
       "      <td>2024</td>\n",
       "      <td>29</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[{'name': 'Hongyang Chen', 'orcid': 'https://o...</td>\n",
       "      <td>[{'name': 'Battery (electricity)', 'level': 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1109/TEVC.2023.3278132</td>\n",
       "      <td>https://openalex.org/W4381983181</td>\n",
       "      <td>Knowledge Learning for Evolutionary Computation</td>\n",
       "      <td>2023</td>\n",
       "      <td>27</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[{'name': 'Yi Jiang', 'orcid': 'https://orcid....</td>\n",
       "      <td>[{'name': 'Computer science', 'level': 0, 'sco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          doi                       openalex_id  \\\n",
       "0   10.1177/10963480241229235  https://openalex.org/W4391822953   \n",
       "1      10.1002/adfm.202413884  https://openalex.org/W4403192979   \n",
       "2  10.1109/TNNLS.2023.3336563  https://openalex.org/W4391130239   \n",
       "3  10.1016/j.esci.2024.100281  https://openalex.org/W4398247259   \n",
       "4   10.1109/TEVC.2023.3278132  https://openalex.org/W4381983181   \n",
       "\n",
       "                                               title  publication_year  \\\n",
       "0  Artificial Intelligence in Hospitality and Tou...              2024   \n",
       "1  Hierarchical Engineering on Built‐In Electric ...              2024   \n",
       "2  Learning to Aggregate Multi-Scale Context for ...              2024   \n",
       "3  Heterogeneous structure design for stable Li/N...              2024   \n",
       "4    Knowledge Learning for Evolutionary Computation              2023   \n",
       "\n",
       "   cited_by_count journal journal_issn_l  \\\n",
       "0              30     N/A            N/A   \n",
       "1              26     N/A            N/A   \n",
       "2              33     N/A            N/A   \n",
       "3              29     N/A            N/A   \n",
       "4              27     N/A            N/A   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'name': 'Hyunsu Kim', 'orcid': 'https://orci...   \n",
       "1  [{'name': 'Shijie Zhang', 'orcid': 'https://or...   \n",
       "2  [{'name': 'Ye Liu', 'orcid': 'https://orcid.or...   \n",
       "3  [{'name': 'Hongyang Chen', 'orcid': 'https://o...   \n",
       "4  [{'name': 'Yi Jiang', 'orcid': 'https://orcid....   \n",
       "\n",
       "                                            concepts  \n",
       "0  [{'name': 'Hospitality', 'level': 3, 'score': ...  \n",
       "1  [{'name': 'Materials science', 'level': 0, 'sc...  \n",
       "2  [{'name': 'Computer science', 'level': 0, 'sco...  \n",
       "3  [{'name': 'Battery (electricity)', 'level': 3,...  \n",
       "4  [{'name': 'Computer science', 'level': 0, 'sco...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1142bd",
   "metadata": {},
   "source": [
    "### OpenAlex - PolyU's Institutional Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d45851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful url: \n",
    "# Find your intitution ID\n",
    "'https://api.openalex.org/institutions?search=hong%kong%polytechnic%university'\n",
    "\n",
    "# OpenAlex Institute ID: https://openalex.org/I14243506\n",
    "# ROR ID: https://ror.org/0030zas98 \n",
    "\n",
    "# All works of the institution\n",
    "'https://api.openalex.org/works?filter=institutions.id:https://openalex.org/I14243506'\n",
    "\n",
    "# Works for specific years\n",
    "'https://api.openalex.org/works?filter=institutions.id:https://openalex.org/I14243506,publication_year:2020-2025&sort=publication_date:desc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232e0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json # For handling complex data structures if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c59b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_normalize_openalex_data(api_url, csv_filename=\"output_publications.csv\"):\n",
    "    \"\"\"\n",
    "    Fetches publication data from the OpenAlex API using pagination,\n",
    "    normalizes the JSON results, saves them to a CSV file, and returns\n",
    "    a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        api_url (str): The base OpenAlex API URL with filters and sorting.\n",
    "                       Pagination parameters ('page', 'per_page') will be added.\n",
    "        csv_filename (str): The name for the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame or None: A DataFrame containing the normalized\n",
    "                                  publication data, or None if an error occurs during fetch.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    page = 1\n",
    "    # Use a larger page size for fewer requests (OpenAlex max is 200)\n",
    "    per_page = 200 \n",
    "    \n",
    "    # IMPORTANT: Add your email to the headers for API politeness (as recommended by OpenAlex)\n",
    "    # Replace 'your_email@example.com' with your actual email address.\n",
    "    headers = {\n",
    "        'User-Agent': 'PolyUAnalysisPortfolioProject/0.1 (mailto:bennis.yiu@connect.polyu.hk)',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting data fetch from OpenAlex...\")\n",
    "    print(f\"Base URL: {api_url}\")\n",
    "    print(\"INFO: Using headers with User-Agent and mailto. PLEASE REPLACE 'bennis.yiu@connect.polyu.hk' in the code with your actual email.\")\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL with pagination parameters\n",
    "        paginated_url = f\"{api_url}&page={page}&per_page={per_page}\"\n",
    "        print(f\"Fetching page {page} (up to {per_page} results per page)...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(paginated_url, headers=headers, timeout=60) # Increased timeout\n",
    "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "            \n",
    "            # Handle potential empty response or non-JSON response gracefully\n",
    "            if not response.content:\n",
    "                print(f\"Warning: Received empty response for page {page}. Assuming end of results.\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                data = response.json()\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode JSON from response for page {page}.\")\n",
    "                print(\"Response first 500 chars:\", response.text[:500])\n",
    "                # Decide how to handle: break, continue, or return None\n",
    "                print(\"Stopping data fetch due to JSON decode error.\")\n",
    "                return None # Or potentially return df with data fetched so far\n",
    "\n",
    "            results = data.get('results', [])\n",
    "\n",
    "            if not results:\n",
    "                print(\"No more results found on this page. Fetching complete.\")\n",
    "                break # Exit loop if no results on this page\n",
    "\n",
    "            all_results.extend(results)\n",
    "            total_fetched = len(all_results)\n",
    "            total_count = data.get('meta', {}).get('count', 'unknown')\n",
    "            print(f\"Fetched {len(results)} results from page {page}. Total results so far: {total_fetched} / {total_count}\")\n",
    "\n",
    "            page += 1\n",
    "            time.sleep(0.2) # Be polite to the API - wait 200ms between requests\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Request timed out on page {page}. Retrying after a longer wait...\")\n",
    "            time.sleep(10) # Wait longer before retry\n",
    "            continue # Retry the same page\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data from {paginated_url}: {e}\")\n",
    "            print(\"Please check the URL, your internet connection, and API rate limits.\")\n",
    "            # Optionally return the data fetched so far, or None\n",
    "            if all_results:\n",
    "                 print(\"Returning data fetched up to the point of error.\")\n",
    "                 break # Exit loop and process collected data\n",
    "            else:\n",
    "                 return None # Return None if no data was fetched before error\n",
    "        except Exception as e: # Catch unexpected errors\n",
    "            print(f\"An unexpected error occurred during fetching page {page}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"No data was fetched or collected.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    print(f\"\\nTotal results fetched: {len(all_results)}\")\n",
    "    print(\"Normalizing data into a DataFrame...\")\n",
    "\n",
    "    try:\n",
    "        # Use pandas json_normalize for flattening the structure\n",
    "        # It handles nested dicts well, creating columns like 'author.name'\n",
    "        df = pd.json_normalize(all_results)\n",
    "\n",
    "        print(f\"Initial normalization resulted in {df.shape[1]} columns.\")\n",
    "\n",
    "        # --- Optional Data Cleaning & Simplification ---\n",
    "        # Some fields like 'authorships', 'concepts', 'locations' etc. remain complex \n",
    "        # (lists of dictionaries). You might want to extract specific info from them.\n",
    "        \n",
    "        # Example: Extract author display names into a comma-separated string\n",
    "        if 'authorships' in df.columns:\n",
    "            try:\n",
    "                df['author_names'] = df['authorships'].apply(\n",
    "                    lambda x: ', '.join([auth.get('author', {}).get('display_name', '') \n",
    "                                         for auth in x if isinstance(auth, dict)]) \n",
    "                    if isinstance(x, list) else None\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not extract author names cleanly: {e}\")\n",
    "                df['author_names'] = None\n",
    "\n",
    "\n",
    "        # Example: Extract concept display names\n",
    "        if 'concepts' in df.columns:\n",
    "            try:\n",
    "                df['concept_names'] = df['concepts'].apply(\n",
    "                    lambda x: ', '.join([concept.get('display_name', '') \n",
    "                                         for concept in x if isinstance(concept, dict) and concept.get('level', 10) == 0]) # Top level concepts only\n",
    "                    if isinstance(x, list) else None\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not extract concept names cleanly: {e}\")\n",
    "                df['concept_names'] = None\n",
    "\n",
    "        # Example: Extract primary source display name\n",
    "        if 'primary_location.source.display_name' in df.columns:\n",
    "            df['source_display_name'] = df['primary_location.source.display_name']\n",
    "        elif 'primary_location' in df.columns: # Handle cases where source might be missing\n",
    "             df['source_display_name'] = df['primary_location'].apply(lambda x: x.get('source', {}).get('display_name') if isinstance(x, dict) else None)\n",
    "        else:\n",
    "             df['source_display_name'] = None\n",
    "             \n",
    "        # Example: Simplify institutions from authorships\n",
    "        if 'authorships' in df.columns:\n",
    "            try:\n",
    "                df['institution_names'] = df['authorships'].apply(\n",
    "                    lambda x: list(set( # Use set to get unique names\n",
    "                        inst.get('display_name', '') \n",
    "                        for auth in x if isinstance(auth, dict) and isinstance(auth.get('institutions'), list) \n",
    "                        for inst in auth['institutions'] if isinstance(inst, dict)\n",
    "                    )) if isinstance(x, list) else None\n",
    "                )\n",
    "                # Convert list to comma separated string for easier CSV viewing\n",
    "                df['institution_names_str'] = df['institution_names'].apply(lambda x: ', '.join(x) if isinstance(x, list) else None)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not extract institution names cleanly: {e}\")\n",
    "                df['institution_names'] = None\n",
    "                df['institution_names_str'] = None\n",
    "\n",
    "\n",
    "        # Select and potentially reorder columns you care most about\n",
    "        # This makes the final CSV/DataFrame cleaner. Add/remove based on your needs.\n",
    "        core_columns = [\n",
    "            'id', 'doi', 'title', 'display_name', 'publication_year', 'publication_date', \n",
    "            'type', 'cited_by_count', 'is_oa', 'oa_status', \n",
    "            'source_display_name', # Simplified source name\n",
    "            'author_names', # Simplified author names\n",
    "            'concept_names', # Simplified concepts\n",
    "            'institution_names_str', # Simplified institution names\n",
    "            # Add other potentially useful flattened columns from json_normalize:\n",
    "            'primary_location.source.id', 'primary_location.landing_page_url', 'primary_location.pdf_url',\n",
    "            'biblio.volume', 'biblio.issue', 'biblio.first_page', 'biblio.last_page',\n",
    "            'language', 'is_retracted', 'is_paratext',\n",
    "            # You might want to keep some original complex fields for deeper analysis later\n",
    "            # 'authorships', 'concepts', 'locations', 'grants', 'referenced_works', 'related_works' \n",
    "        ]\n",
    "        \n",
    "        # Filter DataFrame to keep only existing core columns + any extra ones created\n",
    "        existing_cols = [col for col in core_columns if col in df.columns]\n",
    "        # Add any columns in df not explicitly mentioned in core_columns (optional)\n",
    "        # extra_cols = [col for col in df.columns if col not in existing_cols]\n",
    "        # df_final = df[existing_cols + extra_cols] \n",
    "        df_final = df[existing_cols] # Keep only selected/created columns for simplicity\n",
    "\n",
    "\n",
    "        print(\"Normalization and simplification complete.\")\n",
    "        print(\"Final DataFrame columns:\", df_final.columns.tolist())\n",
    "        print(\"Final DataFrame shape:\", df_final.shape)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data normalization/processing: {e}\")\n",
    "        print(\"Returning the DataFrame from initial normalization if available, otherwise None.\")\n",
    "        # If normalization itself failed hard, df might not exist\n",
    "        if 'df' in locals():\n",
    "             return df # Return the result of initial json_normalize\n",
    "        else:\n",
    "             return None \n",
    "\n",
    "    try:\n",
    "        print(f\"Saving data to {csv_filename}...\")\n",
    "        # Use utf-8 encoding for compatibility with special characters\n",
    "        df_final.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        print(f\"Data successfully saved to {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the CSV file: {e}\")\n",
    "        # Still return the DataFrame even if saving fails\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc542d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch from OpenAlex...\n",
      "Base URL: https://api.openalex.org/works?filter=institutions.id:https://openalex.org/I14243506,publication_year:2020-2025&sort=publication_date:desc\n",
      "INFO: Using headers with User-Agent and mailto. PLEASE REPLACE 'bennis.yiu@connect.polyu.hk' in the code with your actual email.\n",
      "Fetching page 1 (up to 200 results per page)...\n",
      "Fetched 200 results from page 1. Total results so far: 200 / 44972\n",
      "Fetching page 2 (up to 200 results per page)...\n",
      "Fetched 200 results from page 2. Total results so far: 400 / 44972\n",
      "Fetching page 3 (up to 200 results per page)...\n",
      "Fetched 200 results from page 3. Total results so far: 600 / 44972\n",
      "Fetching page 4 (up to 200 results per page)...\n",
      "Fetched 200 results from page 4. Total results so far: 800 / 44972\n",
      "Fetching page 5 (up to 200 results per page)...\n",
      "Fetched 200 results from page 5. Total results so far: 1000 / 44972\n",
      "Fetching page 6 (up to 200 results per page)...\n",
      "Fetched 200 results from page 6. Total results so far: 1200 / 44972\n",
      "Fetching page 7 (up to 200 results per page)...\n",
      "Fetched 200 results from page 7. Total results so far: 1400 / 44972\n",
      "Fetching page 8 (up to 200 results per page)...\n",
      "Fetched 200 results from page 8. Total results so far: 1600 / 44972\n",
      "Fetching page 9 (up to 200 results per page)...\n",
      "Fetched 200 results from page 9. Total results so far: 1800 / 44972\n",
      "Fetching page 10 (up to 200 results per page)...\n",
      "Fetched 200 results from page 10. Total results so far: 2000 / 44972\n",
      "Fetching page 11 (up to 200 results per page)...\n",
      "Fetched 200 results from page 11. Total results so far: 2200 / 44972\n",
      "Fetching page 12 (up to 200 results per page)...\n",
      "Fetched 200 results from page 12. Total results so far: 2400 / 44972\n",
      "Fetching page 13 (up to 200 results per page)...\n",
      "Fetched 200 results from page 13. Total results so far: 2600 / 44972\n",
      "Fetching page 14 (up to 200 results per page)...\n",
      "Fetched 200 results from page 14. Total results so far: 2800 / 44972\n",
      "Fetching page 15 (up to 200 results per page)...\n",
      "Fetched 200 results from page 15. Total results so far: 3000 / 44972\n",
      "Fetching page 16 (up to 200 results per page)...\n",
      "Fetched 200 results from page 16. Total results so far: 3200 / 44972\n",
      "Fetching page 17 (up to 200 results per page)...\n",
      "Fetched 200 results from page 17. Total results so far: 3400 / 44972\n",
      "Fetching page 18 (up to 200 results per page)...\n",
      "Fetched 200 results from page 18. Total results so far: 3600 / 44972\n",
      "Fetching page 19 (up to 200 results per page)...\n",
      "Fetched 200 results from page 19. Total results so far: 3800 / 44972\n",
      "Fetching page 20 (up to 200 results per page)...\n",
      "Fetched 200 results from page 20. Total results so far: 4000 / 44972\n",
      "Fetching page 21 (up to 200 results per page)...\n",
      "Fetched 200 results from page 21. Total results so far: 4200 / 44972\n",
      "Fetching page 22 (up to 200 results per page)...\n",
      "Fetched 200 results from page 22. Total results so far: 4400 / 44972\n",
      "Fetching page 23 (up to 200 results per page)...\n",
      "Fetched 200 results from page 23. Total results so far: 4600 / 44972\n",
      "Fetching page 24 (up to 200 results per page)...\n",
      "Fetched 200 results from page 24. Total results so far: 4800 / 44972\n",
      "Fetching page 25 (up to 200 results per page)...\n",
      "Fetched 200 results from page 25. Total results so far: 5000 / 44972\n",
      "Fetching page 26 (up to 200 results per page)...\n",
      "Fetched 200 results from page 26. Total results so far: 5200 / 44972\n",
      "Fetching page 27 (up to 200 results per page)...\n",
      "Fetched 200 results from page 27. Total results so far: 5400 / 44972\n",
      "Fetching page 28 (up to 200 results per page)...\n",
      "Fetched 200 results from page 28. Total results so far: 5600 / 44972\n",
      "Fetching page 29 (up to 200 results per page)...\n",
      "Fetched 200 results from page 29. Total results so far: 5800 / 44972\n",
      "Fetching page 30 (up to 200 results per page)...\n",
      "Fetched 200 results from page 30. Total results so far: 6000 / 44972\n",
      "Fetching page 31 (up to 200 results per page)...\n",
      "Fetched 200 results from page 31. Total results so far: 6200 / 44972\n",
      "Fetching page 32 (up to 200 results per page)...\n",
      "Fetched 200 results from page 32. Total results so far: 6400 / 44972\n",
      "Fetching page 33 (up to 200 results per page)...\n",
      "Fetched 200 results from page 33. Total results so far: 6600 / 44972\n",
      "Fetching page 34 (up to 200 results per page)...\n",
      "Fetched 200 results from page 34. Total results so far: 6800 / 44972\n",
      "Fetching page 35 (up to 200 results per page)...\n",
      "Fetched 200 results from page 35. Total results so far: 7000 / 44972\n",
      "Fetching page 36 (up to 200 results per page)...\n",
      "Fetched 200 results from page 36. Total results so far: 7200 / 44972\n",
      "Fetching page 37 (up to 200 results per page)...\n",
      "Fetched 200 results from page 37. Total results so far: 7400 / 44972\n",
      "Fetching page 38 (up to 200 results per page)...\n",
      "Fetched 200 results from page 38. Total results so far: 7600 / 44972\n",
      "Fetching page 39 (up to 200 results per page)...\n",
      "Fetched 200 results from page 39. Total results so far: 7800 / 44972\n",
      "Fetching page 40 (up to 200 results per page)...\n",
      "Fetched 200 results from page 40. Total results so far: 8000 / 44972\n",
      "Fetching page 41 (up to 200 results per page)...\n",
      "Fetched 200 results from page 41. Total results so far: 8200 / 44972\n",
      "Fetching page 42 (up to 200 results per page)...\n",
      "Fetched 200 results from page 42. Total results so far: 8400 / 44972\n",
      "Fetching page 43 (up to 200 results per page)...\n",
      "Fetched 200 results from page 43. Total results so far: 8600 / 44972\n",
      "Fetching page 44 (up to 200 results per page)...\n",
      "Fetched 200 results from page 44. Total results so far: 8800 / 44972\n",
      "Fetching page 45 (up to 200 results per page)...\n",
      "Fetched 200 results from page 45. Total results so far: 9000 / 44972\n",
      "Fetching page 46 (up to 200 results per page)...\n",
      "Fetched 200 results from page 46. Total results so far: 9200 / 44972\n",
      "Fetching page 47 (up to 200 results per page)...\n",
      "Fetched 200 results from page 47. Total results so far: 9400 / 44972\n",
      "Fetching page 48 (up to 200 results per page)...\n",
      "Fetched 200 results from page 48. Total results so far: 9600 / 44972\n",
      "Fetching page 49 (up to 200 results per page)...\n",
      "Fetched 200 results from page 49. Total results so far: 9800 / 44972\n",
      "Fetching page 50 (up to 200 results per page)...\n",
      "Fetched 200 results from page 50. Total results so far: 10000 / 44972\n",
      "Fetching page 51 (up to 200 results per page)...\n",
      "Error fetching data from https://api.openalex.org/works?filter=institutions.id:https://openalex.org/I14243506,publication_year:2020-2025&sort=publication_date:desc&page=51&per_page=200: 403 Client Error: FORBIDDEN for url: https://api.openalex.org/works?filter=institutions.id:https://openalex.org/I14243506,publication_year:2020-2025&sort=publication_date:desc&page=51&per_page=200\n",
      "Please check the URL, your internet connection, and API rate limits.\n",
      "Returning data fetched up to the point of error.\n",
      "\n",
      "Total results fetched: 10000\n",
      "Normalizing data into a DataFrame...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- How to Use ---\n",
    "if __name__ == \"__main__\":\n",
    "    # The URL provided by the user (without pagination parameters)\n",
    "    polyu_api_base_url = \"https://api.openalex.org/works?filter=institutions.id:https://openalex.org/I14243506,publication_year:2020-2025&sort=publication_date:desc\"\n",
    "    \n",
    "    output_csv_file = \"polyu_publications_2020-2025.csv\"\n",
    "\n",
    "    # IMPORTANT: Remember to replace 'your_email@example.com' inside the function's header definition.\n",
    "    \n",
    "    # Call the function to fetch, normalize, save, and get the DataFrame\n",
    "    polyu_dataframe = fetch_and_normalize_openalex_data(polyu_api_base_url, output_csv_file)\n",
    "\n",
    "    if polyu_dataframe is not None:\n",
    "        print(\"\\n--- DataFrame Info ---\")\n",
    "        polyu_dataframe.info()\n",
    "        print(\"\\n--- DataFrame Head (first 5 rows) ---\")\n",
    "        # Display more columns if needed for verification\n",
    "        with pd.option_context('display.max_rows', 5, 'display.max_columns', 15): \n",
    "            print(polyu_dataframe.head())\n",
    "    else:\n",
    "        print(\"\\nProcess failed. No DataFrame was returned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyu_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a052375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643677e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473a45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbee22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

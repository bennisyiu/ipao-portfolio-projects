{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5437eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import pytz\n",
    "import psycopg2\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from datetime import timedelta, datetime\n",
    "from pandas import json_normalize\n",
    "from dotenv import load_dotenv\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeecb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    \"\"\"Load and validate credentials from environment variables.\"\"\"\n",
    "\n",
    "    print('get_credentials() method')\n",
    "    load_dotenv()  # Load .env file\n",
    "\n",
    "    # Facebook credentials\n",
    "    scopus_api_key = os.getenv(\"SCOPUS_API_KEY\")\n",
    "    if not scopus_api_key:\n",
    "        raise ValueError(\"SCOPUS_API_KEY is missing in .env!\")\n",
    "\n",
    "    scopus_credentials = {\n",
    "        \"access_token\": scopus_api_key,\n",
    "        \"scopus_label\": os.getenv(\"SCOPUS_LABEL\")\n",
    "    }\n",
    "\n",
    "    # Database credentials\n",
    "    db_credentials = {\n",
    "        \"hostname\": os.getenv(\"DB_HOST\"),\n",
    "        \"port\": int(os.getenv(\"DB_PORT\")),  # Convert to integer\n",
    "        \"username\": os.getenv(\"DB_USER\"),\n",
    "        \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "        \"database\": os.getenv(\"DB_NAME\"),\n",
    "        \"schema\": os.getenv(\"DB_SCHEMA\")\n",
    "    }\n",
    "\n",
    "    return scopus_credentials, db_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8598d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_api_caller(url, params, headers, max_retries=3, timeout=20):\n",
    "    print('scopus_api_caller() method')\n",
    "    all_data = []\n",
    "    retry_count = 0\n",
    "\n",
    "    while url and retry_count < max_retries:\n",
    "        try:\n",
    "            print(f'Making request to URL: {url}')\n",
    "            response = requests.get(\n",
    "                url, params=params, headers=headers, timeout=timeout)\n",
    "            print(f'Response status code: {response.status_code}')\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f'Error response content: {response.text}')\n",
    "\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if 'search-results' in data and 'entry' in data['search-results']:\n",
    "                all_data.extend(data['search-results']['entry'])\n",
    "                print(\n",
    "                    f\"Collected {len(data['search-results']['entry'])} items. Total: {len(all_data)}\")\n",
    "            else:\n",
    "                print(\"No data found in response\")\n",
    "                break\n",
    "\n",
    "            # Check if there are more pages\n",
    "            if 'link' in data['search-results']:\n",
    "                next_link = next(\n",
    "                    (link for link in data['search-results']['link'] if link['@ref'] == 'next'), None)\n",
    "                if next_link:\n",
    "                    url = next_link['@href']\n",
    "                    params = {}  # Clear params as they're included in the next URL\n",
    "                else:\n",
    "                    url = None\n",
    "                    print(\"No more pages\")\n",
    "            else:\n",
    "                url = None\n",
    "                print(\"No more pages\")\n",
    "\n",
    "            retry_count = 0  # Reset retry count on successful request\n",
    "\n",
    "        except (Timeout, RequestException) as e:\n",
    "            retry_count += 1\n",
    "            print(\n",
    "                f\"Request failed: {e}. Retry attempt {retry_count} of {max_retries}\")\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached. Exiting.\")\n",
    "                break\n",
    "            time.sleep(2 ** retry_count)  # Exponential backoff\n",
    "\n",
    "    print(f'Exiting scopus_api_caller. Total items collected: {len(all_data)}')\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab5a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_search(scopus_credentials, query, start=0, count=25, sort='citedby-count', max_results=5000):\n",
    "    print('scopus_search() method')\n",
    "\n",
    "    scopus_api_key = scopus_credentials['access_token']\n",
    "    url = 'https://api.elsevier.com/content/search/scopus'\n",
    "\n",
    "    headers = {\n",
    "        'X-ELS-APIKey': scopus_api_key,\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    # list of fields\n",
    "    fields = [\n",
    "        # 'link ref=self', # Content Abstract Retrieval API URI\n",
    "        # 'link ref=scopus', # Scopus abstract detail page URL\n",
    "        # 'link ref=scopus-citedby', # Scopus Cited By Results URL\n",
    "        'prism:url', # Content Abstract Retrieval API URI\n",
    "        'dc:identifier',  # Unique identifier: Scopus ID\n",
    "        'eid', # Electronic ID\n",
    "        'dc:title',  # Article Title\n",
    "        'prism:aggregationType', # Source Type\n",
    "        'prism:doi',      # DOI for Abstract Retrieval API\n",
    "        'prism:coverDate',  # Publication date\n",
    "        'citedby-count',  # Citation count\n",
    "        'prism:publicationName',  # Journal or conference name\n",
    "        'affiliation',  # Author affiliation\n",
    "        'prism:isbn', # Source Identifier\n",
    "        'prism:issn', # Source Identifier\n",
    "        'prism:volume', # Volume\n",
    "        'prism:issueIdentifier', # Issue\n",
    "        'prism:pageRange', # Page\n",
    "        'pii', # Publication Item Identifier\n",
    "        'pubmed-id', # MEDLINE Identifier\n",
    "        # 'orcid', # ORCID\n",
    "        'dc:creator', # First Author\n",
    "        'subtype', # Document Type code\n",
    "        'subtypeDescription', # Type of publication (e.g., Article, Conference Paper)\n",
    "        'openaccess' # Open Access Status\n",
    "    ]\n",
    "\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'field': ','.join(fields),\n",
    "        'count': count,\n",
    "        'start': start,\n",
    "        'sort': sort\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    total_results = None\n",
    "\n",
    "    while len(all_results) < max_results:\n",
    "        print(f'Full URL: {url}')\n",
    "        print(f'Headers: {headers}')\n",
    "        print(f'Params: {params}')\n",
    "\n",
    "        # Fetch the data\n",
    "        batch_results = scopus_api_caller(url, params, headers)\n",
    "\n",
    "        if not batch_results:\n",
    "            print(\"No results returned from API. Stopping search.\")\n",
    "            break\n",
    "\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "        # Check total number of results if not already set\n",
    "        if total_results is None:\n",
    "            total_results = int(batch_results[0].get(\n",
    "                'search-results', {}).get('opensearch:totalResults', 0))\n",
    "            print(f\"Total results available: {total_results}\")\n",
    "            if total_results == 0:\n",
    "                print(\"No results found for the given query.\")\n",
    "                break\n",
    "\n",
    "        # Update start for the next page\n",
    "        params['start'] = len(all_results)\n",
    "\n",
    "        # Check if we've reached the end of results\n",
    "        if len(all_results) >= total_results or len(all_results) >= max_results:\n",
    "            print(\"All available results have been retrieved or max results reached.\")\n",
    "            break\n",
    "\n",
    "        if len(batch_results) < count:\n",
    "            print(\"Reached the end of available results.\")\n",
    "            break\n",
    "\n",
    "    print(f'{len(all_results)} of SCOPUS data will be processed.')\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"No results found for the given query.\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b11189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list_item(item):\n",
    "    if isinstance(item, dict):\n",
    "        return [str(value) for value in item.values() if value]\n",
    "    elif isinstance(item, str):\n",
    "        return [item]\n",
    "    elif isinstance(item, list):\n",
    "        return [str(subitem) for subitem in item if subitem]\n",
    "    else:\n",
    "        return [str(item)] if item else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b50dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_scopus_search_results(all_data):\n",
    "#     \"\"\"Process Scopus search results data\n",
    "#     Return a dataframe with selected columns and database-friendly names\n",
    "#     \"\"\"\n",
    "#     if not all_data:\n",
    "#         print(\"No data received from Scopus API\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     df = pd.json_normalize(all_data)\n",
    "\n",
    "#     # Function to clean column names\n",
    "#     def clean_column_name(name):\n",
    "#         # Replace non-alphanumeric characters with underscores\n",
    "#         name = re.sub(r'[^a-zA-Z0-9]', '_', name)\n",
    "#         # Replace multiple underscores with a single underscore\n",
    "#         name = re.sub(r'_+', '_', name)\n",
    "#         # Remove leading or trailing underscores\n",
    "#         name = name.strip('_')\n",
    "#         # Convert to lowercase\n",
    "#         return name.lower()\n",
    "\n",
    "#     # Clean column names\n",
    "#     df.columns = [clean_column_name(col) for col in df.columns]\n",
    "\n",
    "#     # Ensure all fields from scopus_search() are present\n",
    "#     expected_fields = [\n",
    "#         'dc_identifier',\n",
    "#         'prism_doi',\n",
    "#         'prism_coverdate',\n",
    "#         'citedby_count',\n",
    "#         'prism_publicationname',\n",
    "#         'subtype',\n",
    "#         'subtypedescription'\n",
    "#     ]\n",
    "\n",
    "#     for field in expected_fields:\n",
    "#         if field not in df.columns:\n",
    "#             df[field] = None\n",
    "#             print(\n",
    "#                 f\"Warning: '{field}' not found in API response. Added as empty column.\")\n",
    "\n",
    "#     # Convert numeric fields\n",
    "#     if 'citedby_count' in df.columns:\n",
    "#         df['citedby_count'] = pd.to_numeric(\n",
    "#             df['citedby_count'], errors='coerce')\n",
    "\n",
    "#     # Convert date fields\n",
    "#     if 'prism_coverdate' in df.columns:\n",
    "#         df['prism_coverdate'] = pd.to_datetime(\n",
    "#             df['prism_coverdate'], errors='coerce')\n",
    "\n",
    "#     # Add a column for publication year\n",
    "#     if 'prism_coverdate' in df.columns:\n",
    "#         df['publication_year'] = df['prism_coverdate'].dt.year\n",
    "#     else:\n",
    "#         print(\n",
    "#             \"Warning: 'prism_coverdate' not found in the data. Using 2100 as fallback year.\")\n",
    "#         df['publication_year'] = 2100\n",
    "\n",
    "#     # Ensure publication_year is always an integer\n",
    "#     df['publication_year'] = df['publication_year'].fillna(2100).astype(int)\n",
    "\n",
    "#     # Print column names and their types for debugging\n",
    "#     print(\"Column names and types:\")\n",
    "#     print(df.dtypes)\n",
    "\n",
    "#     # Print the first few rows for debugging\n",
    "#     print(\"First few rows of the processed dataframe:\")\n",
    "#     print(df.head())\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f456169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Revised I - process_scopus_search_results()\n",
    "# def process_scopus_search_results(all_data):\n",
    "#     \"\"\"\n",
    "#     Process Scopus search results data based on specifically requested fields.\n",
    "#     Normalizes nested data (affiliation, creator), cleans column names,\n",
    "#     extracts key links, converts types, and adds publication year/month.\n",
    "\n",
    "#     Args:\n",
    "#         all_data (list): List of raw result dictionaries from Scopus API.\n",
    "\n",
    "#     Returns:\n",
    "#         pandas.DataFrame: Processed DataFrame with cleaned names and added columns.\n",
    "#     \"\"\"\n",
    "#     if not all_data:\n",
    "#         print(\"No data received from Scopus API to process.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     print(f\"Processing {len(all_data)} raw Scopus records.\")\n",
    "\n",
    "#     # --- Pre-processing Step: Extract Specific Links ---\n",
    "#     # This makes handling links easier than relying solely on json_normalize\n",
    "#     processed_data = []\n",
    "#     for item in all_data:\n",
    "#         new_item = item.copy() # Work on a copy\n",
    "#         links = new_item.get('link', [])\n",
    "#         if isinstance(links, list):\n",
    "#             for link_info in links:\n",
    "#                 if isinstance(link_info, dict):\n",
    "#                     ref = link_info.get('@ref')\n",
    "#                     href = link_info.get('@href')\n",
    "#                     if ref and href:\n",
    "#                         # Create specific keys for the links we want\n",
    "#                         if ref == 'self':\n",
    "#                             new_item['link_self_href'] = href\n",
    "#                         elif ref == 'scopus':\n",
    "#                             new_item['link_scopus_href'] = href\n",
    "#                         elif ref == 'scopus-citedby':\n",
    "#                             new_item['link_scopus_citedby_href'] = href\n",
    "#             # Remove the original complex 'link' field after extraction\n",
    "#             if 'link' in new_item:\n",
    "#                 del new_item['link']\n",
    "#         processed_data.append(new_item)\n",
    "#     all_data = processed_data # Use the pre-processed data from now on\n",
    "#     # --- End Pre-processing ---\n",
    "\n",
    "\n",
    "#     # Convert to DataFrame using json_normalize to handle potential nested structures\n",
    "#     # esp. for 'affiliation' and 'dc:creator'\n",
    "#     try:\n",
    "#         df = pd.json_normalize(all_data, sep='_') # Use underscore separator\n",
    "#         print(f\"Normalized data into DataFrame shape: {df.shape}\")\n",
    "#         # print(\"Initial columns after normalize:\", df.columns.tolist()) # Debug\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during pandas json_normalize: {e}\")\n",
    "#         print(\"Attempting basic DataFrame creation (may lose nested data).\")\n",
    "#         try:\n",
    "#             df = pd.DataFrame(all_data)\n",
    "#         except Exception as e2:\n",
    "#             print(f\"Basic DataFrame creation also failed: {e2}\")\n",
    "#             return pd.DataFrame()\n",
    "\n",
    "#     # --- Robust Column Name Cleaning Function ---\n",
    "#     def clean_column_name(name):\n",
    "#         name = str(name) # Ensure string\n",
    "#         # Replace problematic characters (:, -, @, .) with underscores\n",
    "#         # Keeping @ handling just in case, although pre-processing links helps\n",
    "#         name = re.sub(r'[:\\-@\\.]', '_', name)\n",
    "#         # Handle CamelCase by inserting underscore before capitals (except first char)\n",
    "#         # name = re.sub(r'(?<!^)(?=[A-Z])', '_', name) # Optional: uncomment if you see CamelCase cols\n",
    "#         # Remove characters that are not alphanumeric or underscore\n",
    "#         name = re.sub(r'[^a-zA-Z0-9_]', '', name)\n",
    "#         # Replace multiple underscores with a single underscore\n",
    "#         name = re.sub(r'_+', '_', name)\n",
    "#         # Remove leading or trailing underscores\n",
    "#         name = name.strip('_')\n",
    "#         # Convert to lowercase\n",
    "#         return name.lower()\n",
    "\n",
    "#     # Clean column names\n",
    "#     df.columns = [clean_column_name(col) for col in df.columns]\n",
    "#     print(f\"Cleaned column names. Shape remains: {df.shape}\")\n",
    "#     # print(\"Cleaned columns:\", df.columns.tolist()) # Debug\n",
    "\n",
    "#     # --- Define expected columns (cleaned names) based on fields requested ---\n",
    "#     # Ensure these match the cleaned versions of fields from scopus_search + pre-processed links\n",
    "#     expected_fields_cleaned = [\n",
    "#         'link_self_href',             # Extracted pre-processing\n",
    "#         'link_scopus_href',           # Extracted pre-processing\n",
    "#         'link_scopus_citedby_href',   # Extracted pre-processing\n",
    "#         'prism_url',\n",
    "#         'dc_identifier',\n",
    "#         'eid',\n",
    "#         'dc_title',\n",
    "#         'prism_aggregationtype',\n",
    "#         'prism_doi',\n",
    "#         'prism_coverdate',\n",
    "#         'citedby_count',\n",
    "#         'prism_publicationname',\n",
    "#         # 'affiliation' itself might not exist after normalize if it was nested\n",
    "#         # Check for normalized affiliation columns instead (e.g., affiliation_0_affilname)\n",
    "#         'prism_isbn',               # Note: ISBN might be list, handled later\n",
    "#         'prism_issn',               # Note: ISSN might be list, handled later\n",
    "#         'prism_volume',\n",
    "#         'prism_issueidentifier',\n",
    "#         'prism_pagerange',\n",
    "#         'pii',\n",
    "#         'pubmed_id',\n",
    "#         'orcid',\n",
    "#         # 'dc_creator' might not exist if nested, check for 'dc_creator_$' or similar\n",
    "#         'subtype',\n",
    "#         'subtypedescription',\n",
    "#         'openaccess',\n",
    "#         # Add publication year/month which are created here\n",
    "#         'publication_year',\n",
    "#         'publication_month'\n",
    "#     ]\n",
    "\n",
    "#     # --- Ensure Core/Expected Columns Exist (add if missing) ---\n",
    "#     # We focus on ensuring columns exist, normalization might create others (like affiliation parts)\n",
    "#     for field in expected_fields_cleaned:\n",
    "#          if field not in df.columns:\n",
    "#             # Don't automatically add affiliation/creator base names if normalized versions exist\n",
    "#             if field not in ['affiliation', 'dc_creator']:\n",
    "#                  # Check if a related normalized column exists (simple check)\n",
    "#                  related_prefix = field.split('_')[0] # e.g., 'link'\n",
    "#                  if not any(col.startswith(related_prefix) for col in df.columns):\n",
    "#                       df[field] = None\n",
    "#                       # print(f\"Warning: Column '{field}' not found and no related cols found. Added as empty.\")\n",
    "\n",
    "\n",
    "#     # --- Type Conversions ---\n",
    "#     numeric_cols = ['citedby_count', 'openaccess', 'pubmed_id'] # pubmed_id is often numeric\n",
    "#     for col in numeric_cols:\n",
    "#         if col in df.columns:\n",
    "#             df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) # Coerce errors to NaN, then fill with 0\n",
    "#             # Convert to integer if no NaNs were introduced or after filling\n",
    "#             # Check if column contains only integer values (or NaN filled as 0)\n",
    "#             if df[col].dropna().mod(1).eq(0).all():\n",
    "#                  df[col] = df[col].astype(int)\n",
    "#             else:\n",
    "#                  df[col] = df[col].astype(float) # Keep as float if decimals exist\n",
    "\n",
    "\n",
    "#     date_cols = ['prism_coverdate']\n",
    "#     for col in date_cols:\n",
    "#         if col in df.columns:\n",
    "#             df[col] = pd.to_datetime(df[col], errors='coerce') # Coerce errors to NaT\n",
    "\n",
    "\n",
    "#     # --- Add Publication Year and Month ---\n",
    "#     if 'prism_coverdate' in df.columns and pd.api.types.is_datetime64_any_dtype(df['prism_coverdate']):\n",
    "#         df['publication_year'] = df['prism_coverdate'].dt.year.fillna(0).astype(int) # Use 0 for missing\n",
    "#         df['publication_month'] = df['prism_coverdate'].dt.month.fillna(0).astype(int) # Use 0 for missing\n",
    "#         print(\"Added 'publication_year' and 'publication_month' columns.\")\n",
    "#     else:\n",
    "#         print(\"Warning: 'prism_coverdate' not found or not in expected date format. Cannot add year/month reliably.\")\n",
    "#         # Ensure columns exist even if calculation fails\n",
    "#         if 'publication_year' not in df.columns: df['publication_year'] = 0\n",
    "#         if 'publication_month' not in df.columns: df['publication_month'] = 0\n",
    "#         df['publication_year'] = df['publication_year'].fillna(0).astype(int)\n",
    "#         df['publication_month'] = df['publication_month'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "#     # --- Handle potential list/dict columns (flatten to string) ---\n",
    "#     # Important for ISBN, ISSN, and potentially affiliation/creator if normalization wasn't perfect\n",
    "#     # Check if ANY value in a column is a list or dict\n",
    "#     for col in df.select_dtypes(include=['object']).columns: # Check only object columns\n",
    "#         if df[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
    "#             print(f\"Column '{col}' contains lists/dicts. Converting to string representation.\")\n",
    "#             # Safely convert to string, handling None/NaN\n",
    "#             df[col] = df[col].apply(lambda x: str(x) if x is not None else None)\n",
    "\n",
    "\n",
    "#     # --- Simplify Affiliation/Author (Example: Extract First) ---\n",
    "#     # This depends on the output of json_normalize. Check your actual column names.\n",
    "#     # Common patterns: 'affiliation_0_affilname', 'dc_creator_$'\n",
    "\n",
    "#     # Attempt to find the first affiliation name column\n",
    "#     affil_name_cols = sorted([col for col in df.columns if 'affiliation' in col and 'affilname' in col])\n",
    "#     if affil_name_cols:\n",
    "#         df['first_affiliation_name'] = df[affil_name_cols[0]]\n",
    "#         print(f\"Extracted first affiliation name into 'first_affiliation_name' from '{affil_name_cols[0]}'.\")\n",
    "\n",
    "#     # Attempt to find the first author name column (often 'dc_creator_$' from normalize)\n",
    "#     creator_col = 'dc_creator_' # Common result from normalize on dc:creator if it's simple text\n",
    "#     if creator_col in df.columns:\n",
    "#          df['first_author_name'] = df[creator_col]\n",
    "#          print(f\"Extracted first author name into 'first_author_name' from '{creator_col}'.\")\n",
    "#     else:\n",
    "#          # Fallback check if dc:creator wasn't normalized or missing\n",
    "#          if 'dc_creator' in df.columns:\n",
    "#               df['first_author_name'] = df['dc_creator'].astype(str) # Convert potential dict/list to string\n",
    "#               print(\"Copied 'dc_creator' to 'first_author_name' (structure unknown).\")\n",
    "\n",
    "\n",
    "#     # --- Final Check and Select Columns ---\n",
    "#     # Optionally, select only the columns you definitively want in the final output\n",
    "#     # This prevents unexpected columns from normalization cluttering the result.\n",
    "#     # Create a final list of desired columns based on expected_fields_cleaned + created ones\n",
    "#     final_columns_to_keep = [\n",
    "#         # Core IDs & Links\n",
    "#         'dc_identifier', 'eid', 'prism_doi', 'pii', 'pubmed_id', 'orcid',\n",
    "#         'link_self_href', 'link_scopus_href', 'link_scopus_citedby_href', 'prism_url',\n",
    "#         # Title & Creator\n",
    "#         'dc_title', 'first_author_name', # Use simplified author name\n",
    "#         # Publication Details\n",
    "#         'prism_publicationname', 'prism_aggregationtype', 'prism_issn', 'prism_isbn',\n",
    "#         'prism_volume', 'prism_issueidentifier', 'prism_pagerange',\n",
    "#         # Dates & Citation\n",
    "#         'prism_coverdate', 'publication_year', 'publication_month', 'prism_coverdisplaydate',\n",
    "#         'citedby_count',\n",
    "#         # Document Type\n",
    "#         'subtype', 'subtypedescription',\n",
    "#         # Affiliation\n",
    "#         'first_affiliation_name', # Use simplified affiliation name\n",
    "#         # (Optionally add more normalized affiliation columns if needed, e.g., 'affiliation_0_city')\n",
    "#         # Open Access\n",
    "#         'openaccess',\n",
    "#         # 'openaccessflag' # Not requested in your fields list, but often comes with 'openaccess'\n",
    "#     ]\n",
    "\n",
    "#     # Filter DataFrame to keep only desired columns, adding missing ones as None\n",
    "#     final_df = pd.DataFrame()\n",
    "#     for col in final_columns_to_keep:\n",
    "#         if col in df.columns:\n",
    "#             final_df[col] = df[col]\n",
    "#         else:\n",
    "#             final_df[col] = None # Add column if it wasn't created/found\n",
    "\n",
    "#     print(\"\\nFinal selected columns:\")\n",
    "#     print(final_df.columns.tolist())\n",
    "\n",
    "#     # Print final column names and types for debugging\n",
    "#     print(\"\\nFinal columns and data types after processing & selection:\")\n",
    "#     print(final_df.dtypes)\n",
    "\n",
    "#     # Print the first few rows for debugging\n",
    "#     print(\"\\nFirst few rows of the final processed dataframe:\")\n",
    "#     print(final_df.head())\n",
    "\n",
    "#     return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d29bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revised II\n",
    "\n",
    "def process_scopus_search_results(all_data):\n",
    "    \"\"\"\n",
    "    Process Scopus search results based on the specifically requested fields.\n",
    "    Normalizes data, cleans column names, ensures requested columns exist,\n",
    "    performs basic type conversions, and adds publication year/month.\n",
    "\n",
    "    Args:\n",
    "        all_data (list): List of raw result dictionaries from Scopus API.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    if not all_data:\n",
    "        print(\"No data received from Scopus API to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Processing {len(all_data)} raw Scopus records.\")\n",
    "\n",
    "    # Normalize potential nested structures (like affiliation, creator)\n",
    "    try:\n",
    "        # Using sep='_' handles nested fields like affiliation_0_affilname\n",
    "        df = pd.json_normalize(all_data, sep='_')\n",
    "        print(f\"Normalized data into DataFrame shape: {df.shape}\")\n",
    "        # print(\"Initial columns after normalize:\", df.columns.tolist()) # Debug\n",
    "    except Exception as e:\n",
    "        print(f\"Error during pandas json_normalize: {e}\")\n",
    "        print(\"Attempting basic DataFrame creation (may lose nested data).\")\n",
    "        try:\n",
    "            df = pd.DataFrame(all_data)\n",
    "        except Exception as e2:\n",
    "            print(f\"Basic DataFrame creation also failed: {e2}\")\n",
    "            return pd.DataFrame() # Cannot process\n",
    "\n",
    "    # --- Column Name Cleaning Function ---\n",
    "    def clean_column_name(name):\n",
    "        name = str(name) # Ensure string\n",
    "        # Replace problematic characters (:, -, @, .) with underscores\n",
    "        name = re.sub(r'[:\\-@\\.]', '_', name)\n",
    "        # Optional: Handle CamelCase if observed in column names\n",
    "        # name = re.sub(r'(?<!^)(?=[A-Z])', '_', name)\n",
    "        # Remove characters that are not alphanumeric or underscore\n",
    "        name = re.sub(r'[^a-zA-Z0-9_]', '', name)\n",
    "        # Replace multiple underscores with a single underscore\n",
    "        name = re.sub(r'_+', '_', name)\n",
    "        # Remove leading or trailing underscores\n",
    "        name = name.strip('_')\n",
    "        # Convert to lowercase\n",
    "        return name.lower()\n",
    "\n",
    "    # Clean column names\n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    print(f\"Cleaned column names. Shape remains: {df.shape}\")\n",
    "    # print(\"Cleaned columns:\", df.columns.tolist()) # Debug\n",
    "\n",
    "    # --- Define cleaned names of fields requested in scopus_search ---\n",
    "    # List reflects the LATEST fields list provided in scopus_search\n",
    "    requested_fields_cleaned = [\n",
    "        'prism_url',\n",
    "        'dc_identifier',\n",
    "        'eid',\n",
    "        'dc_title',\n",
    "        'prism_aggregationtype',\n",
    "        'prism_doi',\n",
    "        'prism_coverdate',\n",
    "        'citedby_count',\n",
    "        'prism_publicationname',\n",
    "        # 'affiliation' will likely be normalized, e.g., affiliation_0_affilname\n",
    "        'prism_isbn',\n",
    "        'prism_issn',\n",
    "        'prism_volume',\n",
    "        'prism_issueidentifier',\n",
    "        'prism_pagerange',\n",
    "        'pii',\n",
    "        'pubmed_id',\n",
    "        # 'dc_creator' might be normalized, e.g., dc_creator_$\n",
    "        'subtype',\n",
    "        'subtypedescription',\n",
    "        'openaccess',\n",
    "        # Add derived fields\n",
    "        'publication_year',\n",
    "        'publication_month'\n",
    "    ]\n",
    "\n",
    "    # --- Ensure columns related to requested fields exist ---\n",
    "    # This adds the column with None if neither the base name nor any normalized version exists\n",
    "    base_requested_fields = [ # Original names from your fields list\n",
    "        'prism:url', 'dc:identifier', 'eid', 'dc:title', 'prism:aggregationType',\n",
    "        'prism:doi', 'prism:coverDate', 'citedby-count', 'prism:publicationName',\n",
    "        'affiliation', 'prism:isbn', 'prism:issn', 'prism:volume',\n",
    "        'prism:issueIdentifier', 'prism:pageRange', 'pii', 'pubmed-id',\n",
    "        'dc:creator', 'subtype', 'subtypeDescription', 'openaccess'\n",
    "    ]\n",
    "    for field_original in base_requested_fields:\n",
    "        field_cleaned = clean_column_name(field_original)\n",
    "        # Check if the cleaned name or any column starting with it (due to normalize) exists\n",
    "        if field_cleaned not in df.columns and not any(col.startswith(field_cleaned + '_') for col in df.columns):\n",
    "             # Specifically handle affiliation/creator base names - don't add if normalized versions exist\n",
    "             if field_cleaned in ['affiliation', 'dc_creator'] and any(col.startswith(field_cleaned + '_') for col in df.columns):\n",
    "                  continue # Don't add the base name 'affiliation' if 'affiliation_0_...' exists\n",
    "             else:\n",
    "                  df[field_cleaned] = None\n",
    "                  # print(f\"Warning: Column for '{field_original}' (cleaned: '{field_cleaned}') not found. Added as empty.\")\n",
    "\n",
    "\n",
    "    # --- Basic Type Conversions ---\n",
    "    numeric_cols = ['citedby_count', 'openaccess', 'pubmed_id']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # Convert to numeric, coercing errors; fill resulting NaNs with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "            # Attempt conversion to integer if appropriate\n",
    "            if df[col].dropna().mod(1).eq(0).all():\n",
    "                 try:\n",
    "                     df[col] = df[col].astype(int)\n",
    "                 except pd.errors.IntCastingNaNError: # Should not happen after fillna(0) but safety first\n",
    "                     df[col] = df[col].astype(float) # Keep as float if conversion fails\n",
    "            else:\n",
    "                 df[col] = df[col].astype(float) # Keep as float if decimals exist\n",
    "\n",
    "    date_cols = ['prism_coverdate']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce') # Coerce errors to NaT\n",
    "\n",
    "\n",
    "    # --- Add Publication Year and Month ---\n",
    "    if 'prism_coverdate' in df.columns and pd.api.types.is_datetime64_any_dtype(df['prism_coverdate']):\n",
    "        df['publication_year'] = df['prism_coverdate'].dt.year.fillna(0).astype(int) # Use 0 for missing\n",
    "        df['publication_month'] = df['prism_coverdate'].dt.month.fillna(0).astype(int) # Use 0 for missing\n",
    "        print(\"Added 'publication_year' and 'publication_month' columns.\")\n",
    "    else:\n",
    "        print(\"Warning: 'prism_coverdate' not found or not in expected date format. Cannot add year/month reliably.\")\n",
    "        if 'publication_year' not in df.columns: df['publication_year'] = 0\n",
    "        if 'publication_month' not in df.columns: df['publication_month'] = 0\n",
    "        # Ensure integer type even if defaults are used\n",
    "        df['publication_year'] = df['publication_year'].fillna(0).astype(int)\n",
    "        df['publication_month'] = df['publication_month'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    # --- Convert Remaining List/Dict Columns to String ---\n",
    "    # Checks only object columns for efficiency\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        # Check if *any* non-null entry in the column is a list or dict\n",
    "        if df[col].dropna().apply(lambda x: isinstance(x, (list, dict))).any():\n",
    "            print(f\"Column '{col}' contains lists/dicts. Converting to string representation.\")\n",
    "            # Convert lists/dicts to string, leave others (like simple strings, numbers) as they are\n",
    "            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, (list, dict)) else x)\n",
    "            # Handle potential None values that were not lists/dicts\n",
    "            df[col] = df[col].astype(str).replace({'None': None, 'nan': None}) # Convert actual strings 'None'/'nan' back if needed\n",
    "\n",
    "    # --- Final Output ---\n",
    "    print(\"\\nFinal columns and data types after processing:\")\n",
    "    # Displaying dtypes gives a good overview of the result\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nFirst few rows of the processed dataframe:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Return the DataFrame with all columns resulting from normalization and processing\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f457d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_existing_results(new_results, existing_df):\n",
    "    if existing_df.empty:\n",
    "        return new_results\n",
    "\n",
    "    existing_ids = set(existing_df['dc_identifier'].tolist())\n",
    "    return [result for result in new_results if result.get('dc:identifier') not in existing_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b856029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scopus_research_procedures(years_to_process):\n",
    "#     try:\n",
    "#         scopus_credentials, db_credentials = get_credentials()  # pylint: disable=unused-variable\n",
    "\n",
    "#         max_results_per_api_call = 5000\n",
    "\n",
    "#         csv_file = 'polyu_research_output.csv'\n",
    "#         existing_df = pd.DataFrame()\n",
    "#         try:\n",
    "#             existing_df = pd.read_csv(csv_file)\n",
    "#             if 'publication_year' not in existing_df.columns:\n",
    "#                 print(\"Adding missing publication_year column to existing data\")\n",
    "#                 existing_df['publication_year'] = pd.to_datetime(existing_df['prism_coverdate']).dt.year\n",
    "#             print(f\"Loaded {len(existing_df)} existing records from {csv_file}\")\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"No existing file found at {csv_file}. Starting fresh.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading CSV file: {e}\")\n",
    "\n",
    "#         all_new_results = []\n",
    "#         latest_df = existing_df.copy()  # Initialize latest_df with existing data\n",
    "\n",
    "#         for year in years_to_process:\n",
    "#             query = f\"AFFIL(\\\"The Hong Kong Polytechnic University\\\") AND PUBYEAR = {year}\"\n",
    "#             print(f\"\\nExecuting Scopus search with query: {query}\")\n",
    "\n",
    "#             year_results = []\n",
    "#             start = 0\n",
    "#             while True:\n",
    "#                 try:\n",
    "#                     polyu_results = scopus_search(\n",
    "#                         scopus_credentials, query, start=start, max_results=max_results_per_api_call)\n",
    "\n",
    "#                     if not polyu_results:\n",
    "#                         print(f\"No more results found for {year}.\")\n",
    "#                         break\n",
    "\n",
    "#                     year_results.extend(polyu_results)\n",
    "#                     start += len(polyu_results)\n",
    "\n",
    "#                     print(f\"Retrieved {len(polyu_results)} results for {year}. Total for {year}: {len(year_results)}\")\n",
    "\n",
    "#                     if len(polyu_results) < max_results_per_api_call:\n",
    "#                         print(f\"Reached the end of available results for {year}.\")\n",
    "#                         break\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error during API call: {e}\")\n",
    "#                     print(\"Saving current results and moving to next year.\")\n",
    "#                     break\n",
    "\n",
    "#             # Process and save results for this year\n",
    "#             if year_results:\n",
    "#                 try:\n",
    "#                     new_df = process_scopus_search_results(year_results)\n",
    "#                     if 'publication_year' not in new_df.columns:\n",
    "#                         # Ensure publication_year is added\n",
    "#                         new_df['publication_year'] = year\n",
    "\n",
    "#                     # Combine with existing data\n",
    "#                     latest_df = pd.concat([latest_df, new_df], ignore_index=True)\n",
    "#                     latest_df.drop_duplicates(subset='dc_identifier', keep='last', inplace=True)\n",
    "\n",
    "#                     # Try to save the results to a CSV file\n",
    "#                     try:\n",
    "#                         latest_df.to_csv(csv_file, index=False)\n",
    "#                         print(f\"\\nResults saved to '{csv_file}'. Total records: {len(latest_df)}\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error saving CSV file: {e}\")\n",
    "#                         print(\"Continuing with in-memory DataFrame.\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing results for {year}: {e}\")\n",
    "\n",
    "#             all_new_results.extend(year_results)\n",
    "\n",
    "#         if not all_new_results:\n",
    "#             print(\"No new results to process across all years.\")\n",
    "#         else:\n",
    "#             print(f\"Total new results across all years: {len(all_new_results)}\")\n",
    "\n",
    "#         return latest_df  # Return the latest DataFrame\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return pd.DataFrame()  # Return an empty DataFrame in case of overall failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a63d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revised I - parameters: publication year + list of subtypedescription\n",
    "\n",
    "def scopus_research_procedures(publication_year, document_types):\n",
    "    \"\"\"\n",
    "    Fetches Scopus research output for PolyU for a specific year and\n",
    "    list of document types, handling pagination via scopus_search,\n",
    "    adds publication month, and saves to a year-specific CSV.\n",
    "\n",
    "    Args:\n",
    "        publication_year (int): The year to fetch publications for.\n",
    "        document_types (list): A list of strings representing the\n",
    "                                'subtypeDescription' values to query\n",
    "                                (e.g., ['Article', 'Conference Paper']).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the combined and deduplicated\n",
    "                          results for the specified year and document types,\n",
    "                          including a 'publication_month' column.\n",
    "                          Returns an empty DataFrame on major failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scopus_credentials, _ = get_credentials() # Assuming db_credentials not needed here\n",
    "\n",
    "        # Define the maximum results limit PER document type search for the given year\n",
    "        # This limit is passed down to scopus_search\n",
    "        max_results_per_type_query = 5000\n",
    "\n",
    "        # Use a year-specific CSV file for loading/saving\n",
    "        csv_file = f'polyu_research_output_{publication_year}.csv'\n",
    "        existing_df = pd.DataFrame()\n",
    "        try:\n",
    "            existing_df = pd.read_csv(csv_file)\n",
    "            # Ensure necessary columns exist from previous runs when loading\n",
    "            # Note: process_scopus_search_results already adds publication_year\n",
    "            # We need to potentially add publication_month if it's missing from old file\n",
    "            if 'prism_coverdate' in existing_df.columns and 'publication_month' not in existing_df.columns:\n",
    "                 print(f\"Adding missing 'publication_month' column to existing data in {csv_file}\")\n",
    "                 # Ensure prism_coverdate is datetime before extracting month\n",
    "                 existing_df['prism_coverdate'] = pd.to_datetime(existing_df['prism_coverdate'], errors='coerce')\n",
    "                 existing_df['publication_month'] = existing_df['prism_coverdate'].dt.month.fillna(0).astype(int) # Use 0 for missing month\n",
    "            print(f\"Loaded {len(existing_df)} existing records from {csv_file}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No existing file found at {csv_file}. Starting fresh for year {publication_year}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file {csv_file}: {e}. Starting fresh for year {publication_year}.\")\n",
    "            existing_df = pd.DataFrame() # Ensure it's an empty DF if read fails\n",
    "\n",
    "        # List to hold all *raw* results collected for this year across all specified types\n",
    "        all_new_raw_results_for_year = []\n",
    "\n",
    "        # --- Loop through each document type for the given year ---\n",
    "        for doc_type in document_types:\n",
    "            # Construct the Scopus query for the specific year and document type\n",
    "            query = f'AFFIL(\"The Hong Kong Polytechnic University\") AND PUBYEAR = {publication_year} AND SUBTYPE(\"{doc_type}\")'\n",
    "            print(f\"\\nExecuting Scopus search for Year: {publication_year}, Type: '{doc_type}'\")\n",
    "            print(f\"Query: {query}\")\n",
    "\n",
    "            try:\n",
    "                # Call your existing scopus_search function.\n",
    "                # It internally handles pagination using scopus_api_caller up to max_results.\n",
    "                # We start from 0 for each document type query.\n",
    "                type_results = scopus_search(\n",
    "                    scopus_credentials,\n",
    "                    query,\n",
    "                    start=0,\n",
    "                    # count=25, # Use default count from scopus_search\n",
    "                    # sort='citedby-count', # Use default sort from scopus_search\n",
    "                    max_results=max_results_per_type_query # Pass the limit\n",
    "                )\n",
    "\n",
    "                if type_results:\n",
    "                    print(f\"Retrieved {len(type_results)} raw results for '{doc_type}'.\")\n",
    "                    # Extend the list of raw results for the year\n",
    "                    all_new_raw_results_for_year.extend(type_results)\n",
    "                else:\n",
    "                    # scopus_search already prints messages if no results are found\n",
    "                    print(f\"No results returned by scopus_search for '{doc_type}'.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log error if scopus_search fails for a specific type, but continue\n",
    "                print(f\"Error during scopus_search call for {doc_type} (Year: {publication_year}): {e}\")\n",
    "                print(f\"Skipping document type '{doc_type}' and continuing...\")\n",
    "                # Optionally add more detailed logging here if needed\n",
    "                # traceback.print_exc() # Uncomment for full traceback during debugging\n",
    "\n",
    "        # --- Process all collected raw results for the year ---\n",
    "        if not all_new_raw_results_for_year:\n",
    "            print(f\"\\nNo new raw results collected for year {publication_year} across specified document types.\")\n",
    "            # Return the DataFrame loaded at the start (might be empty or contain previous data)\n",
    "            return existing_df\n",
    "        else:\n",
    "            print(f\"\\nTotal new raw results collected across all types for {publication_year}: {len(all_new_raw_results_for_year)}\")\n",
    "\n",
    "            # --- Process the combined raw list using your existing function ---\n",
    "            try:\n",
    "                # process_scopus_search_results handles normalization, cleaning, and adds 'publication_year'\n",
    "                new_df = process_scopus_search_results(all_new_raw_results_for_year)\n",
    "\n",
    "                if not new_df.empty:\n",
    "                    # --- Add the new 'publication_month' column ---\n",
    "                    if 'prism_coverdate' in new_df.columns:\n",
    "                         # Your process_scopus_search_results already converts prism_coverdate to datetime\n",
    "                         # Extract month, fill NaT/NaN with 0, convert to int\n",
    "                         new_df['publication_month'] = new_df['prism_coverdate'].dt.month.fillna(0).astype(int)\n",
    "                         print(\"Added 'publication_month' column to new data.\")\n",
    "                    else:\n",
    "                         # This case should ideally not happen if prism_coverdate is always requested and processed\n",
    "                         print(\"Warning: 'prism_coverdate' column not found in processed DataFrame. Cannot add 'publication_month'.\")\n",
    "                         new_df['publication_month'] = 0 # Add column with default value\n",
    "\n",
    "                    # --- Combine with existing data loaded earlier ---\n",
    "                    # Ensure columns match if needed, but concat handles differences by creating NaNs\n",
    "                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "                    # --- Deduplicate based on Scopus ID ---\n",
    "                    # Use dc_identifier (cleaned name from process_scopus_search_results)\n",
    "                    if 'dc_identifier' in combined_df.columns:\n",
    "                        initial_rows = len(combined_df)\n",
    "                        # Keep the 'last' entry, assuming newer fetches might have updated info (like citations)\n",
    "                        combined_df.drop_duplicates(subset='dc_identifier', keep='last', inplace=True)\n",
    "                        dedup_rows = len(combined_df)\n",
    "                        print(f\"Deduplicated records based on 'dc_identifier'. Kept {dedup_rows} out of {initial_rows} total records.\")\n",
    "                    else:\n",
    "                        print(\"Warning: 'dc_identifier' column not found in combined DataFrame. Cannot deduplicate effectively.\")\n",
    "\n",
    "\n",
    "                    # --- Save updated data to the year-specific CSV ---\n",
    "                    try:\n",
    "                        combined_df.to_csv(csv_file, index=False)\n",
    "                        print(f\"\\nResults saved to '{csv_file}'. Total records for {publication_year}: {len(combined_df)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving CSV file '{csv_file}': {e}\")\n",
    "                        print(\"Returning in-memory DataFrame without saving.\")\n",
    "\n",
    "                    return combined_df # Return the latest combined DataFrame for the year\n",
    "                else:\n",
    "                     print(\"Processing raw results returned an empty DataFrame. No new data to add.\")\n",
    "                     return existing_df # Return the originally loaded data\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during process_scopus_search_results for year {publication_year}: {e}\")\n",
    "                traceback.print_exc() # Print full traceback for processing errors\n",
    "                # Return existing_df as processing failed, safer than returning partial/corrupt data\n",
    "                print(\"Returning existing data due to processing error.\")\n",
    "                return existing_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in scopus_research_procedures: {e}\")\n",
    "        traceback.print_exc() # Print full traceback for unexpected errors\n",
    "        return pd.DataFrame() # Return an empty DataFrame in case of major failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e4e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage (using the modified function) ---\n",
    "\n",
    "# Define the document types based on your image/Scopus fields\n",
    "doc_types_to_process = [\n",
    "    \"Article\", \"Book\", \"Book Chapter\", \"Conference Paper\", \"Data Paper\",\n",
    "    \"Editorial\", \"Erratum\", \"Letter\", \"Note\", \"Retracted\",\n",
    "    \"Review\", \"Short Survey\"\n",
    "]\n",
    "\n",
    "# doc_types_to_process = [\n",
    "#     \"Article\"\n",
    "# ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_year = 2024 # Example: Process data for 2023\n",
    "\n",
    "# print(f\"\\n--- Starting Scopus Processing for Year: {target_year} ---\")\n",
    "\n",
    "# # Call the modified function for the specific year and document types\n",
    "# yearly_data_df = scopus_research_procedures(target_year, doc_types_to_process)\n",
    "\n",
    "# print(f\"\\n--- Processing Finished for Year: {target_year} ---\")\n",
    "\n",
    "# if not yearly_data_df.empty:\n",
    "#     print(f\"Final DataFrame shape for {target_year}: {yearly_data_df.shape}\")\n",
    "#     print(f\"Columns: {yearly_data_df.columns.tolist()}\")\n",
    "#     print(\"\\nSample data (first 5 rows):\")\n",
    "#     print(yearly_data_df.head())\n",
    "#     if 'publication_month' in yearly_data_df.columns:\n",
    "#         print(\"\\nPublication count per month (0 = month unknown/missing):\")\n",
    "#         # Value counts and sort by month index\n",
    "#         print(yearly_data_df['publication_month'].value_counts().sort_index())\n",
    "# else:\n",
    "#     print(f\"No data returned or generated for {target_year}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = scopus_research_procedures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df1, df2):\n",
    "    # Assuming 'dc_identifier' is the unique identifier for each record\n",
    "    duplicates = df1[df1['dc_identifier'].isin(df2['dc_identifier'])]\n",
    "    print(f\"Number of duplicate records: {len(duplicates)}\")\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_credentials, db_credentials = get_credentials() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_search_data_uploader(db_credentials, df, table_name='scopus_search_output'):\n",
    "    \"\"\"\n",
    "    Upload Scopus data to Postgres database.\n",
    "    Creates the table if it doesn't exist, then upserts data based on dc_identifier.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    print('scopus_search_data_uploader() method')\n",
    "    \n",
    "    try:\n",
    "        # Data preprocessing\n",
    "        date_columns = ['prism_coverdate']\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "        df['publication_year'] = df['publication_year'].fillna(9999).astype(int)\n",
    "\n",
    "        # Connect to the Postgres database\n",
    "        conn = psycopg2.connect(\n",
    "            host=db_credentials['hostname'],\n",
    "            database=db_credentials['database'],\n",
    "            user=db_credentials['username'],\n",
    "            password=db_credentials['password'],\n",
    "            port=db_credentials['port'],\n",
    "            connect_timeout=30\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Set the schema\n",
    "        cursor.execute(sql.SQL(\"SET search_path TO {};\").format(\n",
    "            sql.Identifier(db_credentials['schema'])\n",
    "        ))\n",
    "\n",
    "        # Check if table exists, if not create it\n",
    "        cursor.execute(sql.SQL(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {} (\n",
    "                fa BOOLEAN,\n",
    "                prism_url TEXT,\n",
    "                dc_identifier TEXT PRIMARY KEY,\n",
    "                prism_publicationname TEXT,\n",
    "                prism_coverdate DATE,\n",
    "                prism_doi TEXT,\n",
    "                citedby_count INTEGER,\n",
    "                subtype TEXT,\n",
    "                subtypedescription TEXT,\n",
    "                publication_year INTEGER\n",
    "            )\n",
    "        \"\"\").format(sql.Identifier(table_name)))\n",
    "\n",
    "        # Prepare the data for insertion\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Construct the INSERT ... ON CONFLICT DO UPDATE query\n",
    "        insert_query = sql.SQL(\"\"\"\n",
    "            INSERT INTO {} ({})\n",
    "            VALUES %s\n",
    "            ON CONFLICT (dc_identifier) DO UPDATE SET\n",
    "            {}\n",
    "        \"\"\").format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.SQL(', ').join(map(sql.Identifier, columns)),\n",
    "            sql.SQL(', ').join(\n",
    "                sql.SQL(\"{0} = EXCLUDED.{0}\").format(sql.Identifier(col))\n",
    "                for col in columns if col != 'dc_identifier'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Insert or update data in chunks\n",
    "        chunk_size = 50\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            values = [tuple(row) for _, row in chunk.iterrows()]\n",
    "            execute_values(cursor, insert_query, values)\n",
    "            print(f\"Processed chunk of {len(chunk)} records\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Successfully uploaded/updated data for {len(df)} records\")\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test DB connection\n",
    "# import psycopg2\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Retrieve credentials\n",
    "# db_credentials = {\n",
    "#     'hostname': os.getenv('DB_HOST'),\n",
    "#     'port': int(os.getenv('DB_PORT', 5432)),\n",
    "#     'username': os.getenv('DB_USER'),\n",
    "#     'password': os.getenv('DB_PASSWORD'),\n",
    "#     'database': os.getenv('DB_NAME')\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     # Print credentials for debugging\n",
    "#     print(\"Hostname:\", db_credentials['hostname'])\n",
    "#     print(\"Port:\", db_credentials['port'])\n",
    "#     print(\"Username:\", db_credentials['username'])\n",
    "#     print(\"Password:\", \"*****\")\n",
    "#     print(\"Database:\", db_credentials['database'])\n",
    "\n",
    "#     # Attempt to connect to the database\n",
    "#     conn = psycopg2.connect(\n",
    "#         host=db_credentials['hostname'],\n",
    "#         port=db_credentials['port'],\n",
    "#         user=db_credentials['username'],\n",
    "#         password=db_credentials['password'],\n",
    "#         database=db_credentials['database']\n",
    "#     )\n",
    "#     print(\"Connection successful!\")\n",
    "#     conn.close()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error connecting to the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebbc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_search_data_uploader(db_credentials, latest_df, table_name='scopus_search_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d935c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cbebc4b",
   "metadata": {},
   "source": [
    "### Abstract Retrieval API Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb26e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_retrieval(scopus_credentials, doi):\n",
    "    print(f'abstract_retrieval() method for DOI: {doi}')\n",
    "\n",
    "    scopus_api_key = scopus_credentials['access_token']\n",
    "    url = f'https://api.elsevier.com/content/abstract/doi/{doi}'\n",
    "\n",
    "    headers = {\n",
    "        'X-ELS-APIKey': scopus_api_key,\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Response status code: {response.status_code}\")\n",
    "    response.raise_for_status()\n",
    "\n",
    "    json_response = response.json()\n",
    "    # Print first 500 characters\n",
    "    print(\n",
    "        f\"Response structure: {json.dumps(json_response, indent=2)[:500]}...\")\n",
    "\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dbb0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_abstract_retrieval_results(abstract_data):\n",
    "    \"\"\"Process Abstract Retrieval API results\"\"\"\n",
    "    if not abstract_data:\n",
    "        print(\"No data received from Abstract Retrieval API\")\n",
    "        return {}\n",
    "\n",
    "    # Handle case where abstract_data is a list\n",
    "    if isinstance(abstract_data, list):\n",
    "        abstract_data = abstract_data[0] if abstract_data else {}\n",
    "\n",
    "    coredata = abstract_data.get(\n",
    "        'abstracts-retrieval-response', {}).get('coredata', {})\n",
    "\n",
    "    processed_data = {\n",
    "        'dc:identifier': coredata.get('dc:identifier'),\n",
    "        'dc:title': coredata.get('dc:title'),\n",
    "        'prism:doi': coredata.get('prism:doi'),\n",
    "        'prism:coverDate': coredata.get('prism:coverDate'),\n",
    "        'citedby-count': coredata.get('citedby-count'),\n",
    "        'prism:publicationName': coredata.get('prism:publicationName'),\n",
    "        'subtypeDescription': coredata.get('subtypeDescription'),\n",
    "        'prism:volume': coredata.get('prism:volume'),\n",
    "        'prism:issueIdentifier': coredata.get('prism:issueIdentifier'),\n",
    "        'prism:pageRange': coredata.get('prism:pageRange'),\n",
    "        'openaccess': coredata.get('openaccess'),\n",
    "        'pubmed-id': coredata.get('pubmed-id'),\n",
    "    }\n",
    "\n",
    "    # Process affiliation data\n",
    "    affiliations = abstract_data.get(\n",
    "        'abstracts-retrieval-response', {}).get('affiliation', [])\n",
    "    if not isinstance(affiliations, list):\n",
    "        affiliations = [affiliations] if affiliations else []\n",
    "    processed_data['affiliations'] = [\n",
    "        {\n",
    "            'name': aff.get('affilname'),\n",
    "            'city': aff.get('affiliation-city'),\n",
    "            'country': aff.get('affiliation-country')\n",
    "        }\n",
    "        for aff in affiliations\n",
    "    ]\n",
    "\n",
    "    # Process author data\n",
    "    authors = coredata.get('dc:creator', {})\n",
    "    if isinstance(authors, dict):\n",
    "        authors = authors.get('author', [])\n",
    "    if not isinstance(authors, list):\n",
    "        authors = [authors] if authors else []\n",
    "    processed_data['authors'] = [\n",
    "        {\n",
    "            'name': author.get('ce:indexed-name'),\n",
    "            'affiliation': author.get('affiliation', {}).get('@id') if isinstance(author.get('affiliation'), dict) else author.get('affiliation')\n",
    "        }\n",
    "        for author in authors\n",
    "    ]\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43bf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abstract_retrieval_procedures():\n",
    "#     try:\n",
    "#         scopus_credentials, db_credentials = get_credentials()\n",
    "\n",
    "#         # Test DOIs\n",
    "#         test_dois = [\n",
    "#             \"10.1016/j.rcim.2023.102626\",\n",
    "#             \"10.1038/s41467-024-46022-3\",\n",
    "#             \"10.1038/s41586-024-07161-1\",\n",
    "#             \"10.1016/j.xinn.2024.100612\",\n",
    "#             \"10.1016/j.apcatb.2023.123312\",\n",
    "#             \"10.1016/j.apcatb.2023.123335\",\n",
    "#             \"10.1002/adma.202311970\",\n",
    "#             \"10.1109/JIOT.2024.3361173\",\n",
    "#             \"10.1016/j.engstruct.2023.117193\",\n",
    "#             \"10.1002/adma.202310918\",\n",
    "#             \"10.1002/adma.202307404\",\n",
    "#             \"10.1007/s00170-022-10767-2\",\n",
    "#             \"10.1038/s41560-023-01415-4\",\n",
    "#             \"10.1021/acsnano.3c10674\",\n",
    "#             \"10.1002/adma.202300034\",\n",
    "#             \"10.1021/jacs.3c10516\",\n",
    "#             \"10.1109/TEVC.2022.3215743\",\n",
    "#             \"10.1002/adma.202313548\",\n",
    "#             \"10.1016/j.joule.2023.12.009\",\n",
    "#             \"10.1016/j.knosys.2023.111158\"\n",
    "#         ]\n",
    "\n",
    "#         print(f\"Testing abstract retrieval for {len(test_dois)} DOIs\")\n",
    "\n",
    "#         abstract_results = []\n",
    "#         for doi in test_dois:\n",
    "#             try:\n",
    "#                 print(f\"\\nProcessing DOI: {doi}\")\n",
    "#                 abstract_data = abstract_retrieval(scopus_credentials, doi)\n",
    "#                 processed_abstract = process_abstract_retrieval_results(\n",
    "#                     abstract_data)\n",
    "#                 abstract_results.append(processed_abstract)\n",
    "#                 print(\n",
    "#                     f\"Successfully retrieved and processed abstract for DOI: {doi}\")\n",
    "\n",
    "#                 # Print some details of the processed abstract\n",
    "#                 print(\"Abstract details:\")\n",
    "#                 print(f\"Title: {processed_abstract.get('dc:title', 'N/A')}\")\n",
    "#                 print(\n",
    "#                     f\"Publication Name: {processed_abstract.get('prism:publicationName', 'N/A')}\")\n",
    "#                 print(\n",
    "#                     f\"Cover Date: {processed_abstract.get('prism:coverDate', 'N/A')}\")\n",
    "#                 print(\n",
    "#                     f\"Cited by Count: {processed_abstract.get('citedby-count', 'N/A')}\")\n",
    "#                 print(\"---\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing abstract for DOI {doi}: {e}\")\n",
    "\n",
    "#         if not abstract_results:\n",
    "#             print(\"No abstract results to process.\")\n",
    "#             return\n",
    "\n",
    "#         # Convert abstract results to DataFrame\n",
    "#         abstract_df = pd.DataFrame(abstract_results)\n",
    "\n",
    "#         # Print the first few rows of the abstract DataFrame\n",
    "#         print(\"\\nFirst few rows of the abstract DataFrame:\")\n",
    "#         print(abstract_df.head())\n",
    "\n",
    "#         # Print DataFrame info\n",
    "#         print(\"\\nAbstract DataFrame info:\")\n",
    "#         abstract_df.info()\n",
    "\n",
    "#         # Save abstract results to CSV\n",
    "#         abstract_csv_file = 'scopus_abstract_output_test.csv'\n",
    "#         abstract_df.to_csv(abstract_csv_file, index=False)\n",
    "#         print(f\"\\nAbstract results saved to '{abstract_csv_file}'\")\n",
    "        \n",
    "#         return abstract_df\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_retrieval_procedures():\n",
    "    try:\n",
    "        scopus_credentials, db_credentials = get_credentials()\n",
    "\n",
    "        # Load the CSV file with DOIs\n",
    "        csv_file = 'polyu_scopus_search_output_2020_2025.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            print(f\"Loaded {len(df)} records from {csv_file}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\n",
    "                f\"No file found at {csv_file}. Please run scopus_research_procedures first.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file: {e}\")\n",
    "            return\n",
    "\n",
    "        # Get unique DOIs\n",
    "        dois = df['prism_doi'].dropna().unique()\n",
    "        print(f\"Found {len(dois)} unique DOIs to process\")\n",
    "\n",
    "        abstract_results = []\n",
    "        for doi in dois:\n",
    "            try:\n",
    "                abstract_data = abstract_retrieval(scopus_credentials, doi)\n",
    "                processed_abstract = process_abstract_retrieval_results(\n",
    "                    abstract_data)\n",
    "                abstract_results.append(processed_abstract)\n",
    "                print(\n",
    "                    f\"Successfully retrieved and processed abstract for DOI: {doi}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing abstract for DOI {doi}: {e}\")\n",
    "\n",
    "        if not abstract_results:\n",
    "            print(\"No abstract results to process.\")\n",
    "            return\n",
    "\n",
    "        # Convert abstract results to DataFrame\n",
    "        abstract_df = pd.DataFrame(abstract_results)\n",
    "\n",
    "        # Print the first few rows of the abstract DataFrame\n",
    "        print(\"\\nFirst few rows of the abstract DataFrame:\")\n",
    "        print(abstract_df.head())\n",
    "\n",
    "        # Print DataFrame info\n",
    "        print(\"\\nAbstract DataFrame info:\")\n",
    "        abstract_df.info()\n",
    "\n",
    "        # Save abstract results to CSV\n",
    "        abstract_csv_file = 'scopus_abstract_output.csv'\n",
    "        abstract_df.to_csv(abstract_csv_file, index=False)\n",
    "        print(f\"\\nAbstract results saved to '{abstract_csv_file}'\")\n",
    "        \n",
    "        return abstract_df\n",
    "\n",
    "        # Upload abstract results to database\n",
    "        # try:\n",
    "        #     abstract_retrieval_data_uploader(\n",
    "        #         db_credentials, abstract_df, table_name='scopus_abstract_output')\n",
    "        #     print(\"Abstract Retrieval Data has been uploaded to DB.\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error uploading abstract data to database: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99c53a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_credentials() method\n",
      "Testing abstract retrieval for 20 DOIs\n",
      "\n",
      "Processing DOI: 10.1016/j.rcim.2023.102626\n",
      "abstract_retrieval() method for DOI: 10.1016/j.rcim.2023.102626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Morgantown\",\n",
      "        \"affilname\": \"Benjamin M. Statler College of Engineering and Mineral Resources\",\n",
      "        \"affiliation-country\": \"United States\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hangzhou\",\n",
      "        \"affilname\": \"State Key Laboratory of Fluid Power and Mechatronic Systems\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hangzhou\",\n",
      "        \"affilname...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.rcim.2023.102626\n",
      "Abstract details:\n",
      "Title: Human Digital Twin in the context of Industry 5.0\n",
      "Publication Name: Robotics and Computer-Integrated Manufacturing\n",
      "Cover Date: 2024-02-01\n",
      "Cited by Count: 183\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1038/s41467-024-46022-3\n",
      "abstract_retrieval() method for DOI: 10.1038/s41467-024-46022-3\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Chongqing\",\n",
      "        \"affilname\": \"Chongqing Institute of Green and Intelligent Technology\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Los Angeles\",\n",
      "        \"affilname\": \"UCLA Samueli School of Engineering\",\n",
      "        \"affiliation-country\": \"United States\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"Southern University of Scien...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1038/s41467-024-46022-3\n",
      "Abstract details:\n",
      "Title: Rational molecular and device design enables organic solar cells approaching 20% efficiency\n",
      "Publication Name: Nature Communications\n",
      "Cover Date: 2024-12-01\n",
      "Cited by Count: 152\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1038/s41586-024-07161-1\n",
      "abstract_retrieval() method for DOI: 10.1038/s41586-024-07161-1\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"City University of Hong Kong\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Chengdu\",\n",
      "        \"affilname\": \"Southwest Jiaotong University\",\n",
      "        \"affiliation-cou...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1038/s41586-024-07161-1\n",
      "Abstract details:\n",
      "Title: A three-dimensional liquid diode for soft, integrated permeable electronics\n",
      "Publication Name: Nature\n",
      "Cover Date: 2024-04-04\n",
      "Cited by Count: 126\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1016/j.xinn.2024.100612\n",
      "abstract_retrieval() method for DOI: 10.1016/j.xinn.2024.100612\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"Hong Kong Baptist University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Nanjing\",\n",
      "        \"affilname\": \"Nanjing University\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"East Lansing\",\n",
      "        \"affilname\": \"Michigan State University\",\n",
      "        \"affiliation-country\": \"United States\"\n",
      "...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.xinn.2024.100612\n",
      "Abstract details:\n",
      "Title: Emerging contaminants: A One Health perspective\n",
      "Publication Name: Innovation\n",
      "Cover Date: 2024-07-01\n",
      "Cited by Count: 123\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1016/j.apcatb.2023.123312\n",
      "abstract_retrieval() method for DOI: 10.1016/j.apcatb.2023.123312\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Guangzhou\",\n",
      "        \"affilname\": \"Guangdong University of Technology\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      }\n",
      "    ],\n",
      "    \"coredata\": {\n",
      "      \"srctype\": \"j\",\n",
      "      \"eid\": \"2-s2.0-85171646525\",\n",
      "      \"prism:coverDate\": \"2024-02-01\",\n",
      "   ...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.apcatb.2023.123312\n",
      "Abstract details:\n",
      "Title: Surface reconstruction and directed electron transport in NiSe2/MoSe2 Mott-Schottky heterojunction catalysts promote urea-assisted water splitting\n",
      "Publication Name: Applied Catalysis B: Environmental\n",
      "Cover Date: 2024-02-01\n",
      "Cited by Count: 113\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1016/j.apcatb.2023.123335\n",
      "abstract_retrieval() method for DOI: 10.1016/j.apcatb.2023.123335\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Nanjing\",\n",
      "        \"affilname\": \"Nanjing University of Aeronautics and Astronautics\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      }\n",
      "    ],\n",
      "    \"coredata\": {\n",
      "      \"srctype\": \"j\",\n",
      "      \"eid\": \"2-s2.0-85173181345\",\n",
      "      \"prism:coverDate\": \"20...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.apcatb.2023.123335\n",
      "Abstract details:\n",
      "Title: Recent advancements in the use of novel piezoelectric materials for piezocatalytic and piezo-photocatalytic applications\n",
      "Publication Name: Applied Catalysis B: Environmental\n",
      "Cover Date: 2024-02-01\n",
      "Cited by Count: 106\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1002/adma.202311970\n",
      "abstract_retrieval() method for DOI: 10.1002/adma.202311970\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Xi'an\",\n",
      "        \"affilname\": \"State Key Laboratory of Solidification Processing\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      }\n",
      "    ],\n",
      "    \"coredata\": {\n",
      "      \"srctype\": \"j\",\n",
      "      \"prism:issueIdentifier\": \"16\",\n",
      "      \"eid\": \"2-s2.0-85182477...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1002/adma.202311970\n",
      "Abstract details:\n",
      "Title: Co-Self-Assembled Monolayers Modified NiOx for Stable Inverted Perovskite Solar Cells\n",
      "Publication Name: Advanced Materials\n",
      "Cover Date: 2024-04-18\n",
      "Cited by Count: 99\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1109/JIOT.2024.3361173\n",
      "abstract_retrieval() method for DOI: 10.1109/JIOT.2024.3361173\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"Southern University of Science and Technology\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Southampton\",\n",
      "        \"affilname\": \"University of Southampton\",\n",
      "        \"affiliation-country\": \"United Kingdom\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Beijing\",\n",
      "        \"affilname\": \"Beijing University of Posts and Telecommunicatio...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1109/JIOT.2024.3361173\n",
      "Abstract details:\n",
      "Title: Integrated Sensing and Communications: Recent Advances and Ten Open Challenges\n",
      "Publication Name: IEEE Internet of Things Journal\n",
      "Cover Date: 2024-06-01\n",
      "Cited by Count: 93\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1016/j.engstruct.2023.117193\n",
      "abstract_retrieval() method for DOI: 10.1016/j.engstruct.2023.117193\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"Southern University of Science and Technology\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hangzhou\",\n",
      "        \"affilname\": \"Zhejiang University\",\n",
      "        \"affiliation-...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.engstruct.2023.117193\n",
      "Abstract details:\n",
      "Title: Ultra-High-Strength Engineered Cementitious Composites (UHS-ECC) panel reinforced with FRP bar/grid: Development and flexural performance\n",
      "Publication Name: Engineering Structures\n",
      "Cover Date: 2024-03-01\n",
      "Cited by Count: 92\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1002/adma.202310918\n",
      "abstract_retrieval() method for DOI: 10.1002/adma.202310918\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Kunming\",\n",
      "        \"affilname\": \"Kunming University of Science and Technology\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Shanghai\",\n",
      "        \"affilname\": \"Shanghai Advanced Research Institute, Chinese Academy of Sciences\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Braga\",\n",
      "        \"affilname\": \"International Iberian Na...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1002/adma.202310918\n",
      "Abstract details:\n",
      "Title: In Situ Reconstruction of High-Entropy Heterostructure Catalysts for Stable Oxygen Evolution Electrocatalysis under Industrial Conditions\n",
      "Publication Name: Advanced Materials\n",
      "Cover Date: 2024-04-04\n",
      "Cited by Count: 91\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1002/adma.202307404\n",
      "abstract_retrieval() method for DOI: 10.1002/adma.202307404\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Beijing\",\n",
      "        \"affilname\": \"Peking University\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      }\n",
      "    ],\n",
      "    \"coredata\": {\n",
      "      \"srctype\": \"j\",\n",
      "      \"prism:issueIdentifier\": \"6\",\n",
      "      \"eid\": \"2-s2.0-85178412682\",\n",
      "      \"pubmed-id\": \"37870...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1002/adma.202307404\n",
      "Abstract details:\n",
      "Title: Structural Understanding for High-Voltage Stabilization of Lithium Cobalt Oxide\n",
      "Publication Name: Advanced Materials\n",
      "Cover Date: 2024-02-08\n",
      "Cited by Count: 90\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1007/s00170-022-10767-2\n",
      "abstract_retrieval() method for DOI: 10.1007/s00170-022-10767-2\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Los Angeles\",\n",
      "        \"affilname\": \"USC Viterbi School of Engineering\",\n",
      "        \"affiliation-country\": \"United States\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Sharjah\",\n",
      "        \"affilname\": \"University of Sharjah\",\n",
      "        \"affiliation-country\": \"United Arab Emirates\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Jalandhar\",\n",
      "        \"affilname\": \"I.K. Gujral Punjab Technical University, Jalandha...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1007/s00170-022-10767-2\n",
      "Abstract details:\n",
      "Title: Nanofluids application in machining: a comprehensive review\n",
      "Publication Name: International Journal of Advanced Manufacturing Technology\n",
      "Cover Date: 2024-03-01\n",
      "Cited by Count: 82\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1038/s41560-023-01415-4\n",
      "abstract_retrieval() method for DOI: 10.1038/s41560-023-01415-4\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Oxford\",\n",
      "        \"affilname\": \"University of Oxford\",\n",
      "        \"affiliation-country\": \"United Kingdom\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hsinchu\",\n",
      "        \"affilname\": \"National Synchrotron Radiation Research Center\",\n",
      "        \"affiliation-country\": \"Taiwan\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Zhenjiang\",\n",
      "        \"affilname\": \"Jiangsu University\",\n",
      "        \"affiliation-country\": \"Ch...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1038/s41560-023-01415-4\n",
      "Abstract details:\n",
      "Title: Pure-water-fed, electrocatalytic CO2 reduction to ethylene beyond 1,000 h stability at 10 A\n",
      "Publication Name: Nature Energy\n",
      "Cover Date: 2024-01-01\n",
      "Cited by Count: 81\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1021/acsnano.3c10674\n",
      "abstract_retrieval() method for DOI: 10.1021/acsnano.3c10674\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Wuhan\",\n",
      "        \"affilname\": \"Hubei University of Chinese Medicine\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"Hong Kong Baptist University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-co...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1021/acsnano.3c10674\n",
      "Abstract details:\n",
      "Title: Functionalized Nanomaterials Capable of Crossing the Blood-Brain Barrier\n",
      "Publication Name: ACS Nano\n",
      "Cover Date: 2024-01-23\n",
      "Cited by Count: 80\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1002/adma.202300034\n",
      "abstract_retrieval() method for DOI: 10.1002/adma.202300034\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"Shenzhen University\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      }\n",
      "    ],\n",
      "    \"coredata\": {\n",
      "      \"srctype\": \"j\",\n",
      "      \"prism:issueIdentifier\": \"20\",\n",
      "      \"eid\": \"2-s2.0-85161421323\",\n",
      "      \"pubmed-id\": \"3...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1002/adma.202300034\n",
      "Abstract details:\n",
      "Title: Flexible Organic Transistors for Biosensing: Devices and Applications\n",
      "Publication Name: Advanced Materials\n",
      "Cover Date: 2024-05-16\n",
      "Cited by Count: 80\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1021/jacs.3c10516\n",
      "abstract_retrieval() method for DOI: 10.1021/jacs.3c10516\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Singapore City\",\n",
      "        \"affilname\": \"National University of Singapore\",\n",
      "        \"affiliation-country\": \"Singapore\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"The Chinese University of Hong Kong, Shenzhen\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"Southern University of Science and Technol...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1021/jacs.3c10516\n",
      "Abstract details:\n",
      "Title: Ammonia Electrosynthesis from Nitrate Using a Ruthenium-Copper Cocatalyst System: A Full Concentration Range Study\n",
      "Publication Name: Journal of the American Chemical Society\n",
      "Cover Date: 2024-01-10\n",
      "Cited by Count: 80\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1109/TEVC.2022.3215743\n",
      "abstract_retrieval() method for DOI: 10.1109/TEVC.2022.3215743\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Xi'an\",\n",
      "        \"affilname\": \"Xidian University\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hawthorn\",\n",
      "        \"affilname\": \"Swinburne University of Technology\",\n",
      "        \"affiliation-country\": \"Australia\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong ...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1109/TEVC.2022.3215743\n",
      "Abstract details:\n",
      "Title: Evolutionary Multiform Optimization With Two-Stage Bidirectional Knowledge Transfer Strategy for Point Cloud Registration\n",
      "Publication Name: IEEE Transactions on Evolutionary Computation\n",
      "Cover Date: 2024-02-01\n",
      "Cited by Count: 77\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1002/adma.202313548\n",
      "abstract_retrieval() method for DOI: 10.1002/adma.202313548\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"City University of Hong Kong\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Shenzhen\",\n",
      "        \"affilname\": \"City University of Hong Kong Shenzhen Research Institute\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Baoding\",\n",
      "        \"affilname\": \"North China Electric Power University (Ba...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1002/adma.202313548\n",
      "Abstract details:\n",
      "Title: Crystal Phase Engineering of Ultrathin Alloy Nanostructures for Highly Efficient Electroreduction of Nitrate to Ammonia\n",
      "Publication Name: Advanced Materials\n",
      "Cover Date: 2024-04-04\n",
      "Cited by Count: 74\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1016/j.joule.2023.12.009\n",
      "abstract_retrieval() method for DOI: 10.1016/j.joule.2023.12.009\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Los Angeles\",\n",
      "        \"affilname\": \"UCLA Samueli School of Engineering\",\n",
      "        \"affiliation-country\": \"United States\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Guangzhou\",\n",
      "        \"affilname\": \"State Key Laboratory of Luminescent Materials and Devices\",\n",
      "        \"affiliation-country\": \"China\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Thuwal\",\n",
      "        \"affilname\": \"King Abdullah University of ...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.joule.2023.12.009\n",
      "Abstract details:\n",
      "Title: Achieving 19.4% organic solar cell via an in situ formation of p-i-n structure with built-in interpenetrating network\n",
      "Publication Name: Joule\n",
      "Cover Date: 2024-02-21\n",
      "Cited by Count: 72\n",
      "---\n",
      "\n",
      "Processing DOI: 10.1016/j.knosys.2023.111158\n",
      "abstract_retrieval() method for DOI: 10.1016/j.knosys.2023.111158\n",
      "Response status code: 200\n",
      "Response structure: {\n",
      "  \"abstracts-retrieval-response\": {\n",
      "    \"affiliation\": [\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"The Hong Kong Polytechnic University\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      },\n",
      "      {\n",
      "        \"affiliation-city\": \"Hong Kong\",\n",
      "        \"affilname\": \"Centre for Advances in Reliability and Safety\",\n",
      "        \"affiliation-country\": \"Hong Kong\"\n",
      "      }\n",
      "    ],\n",
      "    \"coredata\": {\n",
      "      \"srctype\": \"j\",\n",
      "      \"eid\": \"2-s2.0-85177994382\",\n",
      "      \"prism:coverDate\": \"2...\n",
      "Successfully retrieved and processed abstract for DOI: 10.1016/j.knosys.2023.111158\n",
      "Abstract details:\n",
      "Title: Fault diagnosis in rotating machines based on transfer learning: Literature review\n",
      "Publication Name: Knowledge-Based Systems\n",
      "Cover Date: 2024-01-11\n",
      "Cited by Count: 72\n",
      "---\n",
      "\n",
      "First few rows of the abstract DataFrame:\n",
      "           dc:identifier                                           dc:title  \\\n",
      "0  SCOPUS_ID:85166028251  Human Digital Twin in the context of Industry 5.0   \n",
      "1  SCOPUS_ID:85186172651  Rational molecular and device design enables o...   \n",
      "2  SCOPUS_ID:85188802733  A three-dimensional liquid diode for soft, int...   \n",
      "3  SCOPUS_ID:85191897734    Emerging contaminants: A One Health perspective   \n",
      "4  SCOPUS_ID:85171646525  Surface reconstruction and directed electron t...   \n",
      "\n",
      "                      prism:doi prism:coverDate citedby-count  \\\n",
      "0    10.1016/j.rcim.2023.102626      2024-02-01           183   \n",
      "1    10.1038/s41467-024-46022-3      2024-12-01           152   \n",
      "2    10.1038/s41586-024-07161-1      2024-04-04           126   \n",
      "3    10.1016/j.xinn.2024.100612      2024-07-01           123   \n",
      "4  10.1016/j.apcatb.2023.123312      2024-02-01           113   \n",
      "\n",
      "                            prism:publicationName subtypeDescription  \\\n",
      "0  Robotics and Computer-Integrated Manufacturing             Review   \n",
      "1                           Nature Communications            Article   \n",
      "2                                          Nature            Article   \n",
      "3                                      Innovation             Review   \n",
      "4              Applied Catalysis B: Environmental            Article   \n",
      "\n",
      "  prism:volume prism:issueIdentifier prism:pageRange openaccess pubmed-id  \\\n",
      "0           85                  None            None          0      None   \n",
      "1           15                     1            None          1  38418862   \n",
      "2          628                  8006           84-92          0  38538792   \n",
      "3            5                     4            None          1      None   \n",
      "4          341                  None            None          0      None   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0  [{'name': 'Benjamin M. Statler College of Engi...   \n",
      "1  [{'name': 'Chongqing Institute of Green and In...   \n",
      "2  [{'name': 'City University of Hong Kong', 'cit...   \n",
      "3  [{'name': 'Hong Kong Baptist University', 'cit...   \n",
      "4  [{'name': 'Guangdong University of Technology'...   \n",
      "\n",
      "                                             authors  \n",
      "0  [{'name': 'Wang B.', 'affiliation': [{'@id': '...  \n",
      "1     [{'name': 'Fu J.', 'affiliation': '60008928'}]  \n",
      "2  [{'name': 'Zhang B.', 'affiliation': [{'@id': ...  \n",
      "3  [{'name': 'Wang F.', 'affiliation': [{'@id': '...  \n",
      "4     [{'name': 'Xu X.', 'affiliation': '60007155'}]  \n",
      "\n",
      "Abstract DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   dc:identifier          20 non-null     object\n",
      " 1   dc:title               20 non-null     object\n",
      " 2   prism:doi              20 non-null     object\n",
      " 3   prism:coverDate        20 non-null     object\n",
      " 4   citedby-count          20 non-null     object\n",
      " 5   prism:publicationName  20 non-null     object\n",
      " 6   subtypeDescription     20 non-null     object\n",
      " 7   prism:volume           20 non-null     object\n",
      " 8   prism:issueIdentifier  15 non-null     object\n",
      " 9   prism:pageRange        8 non-null      object\n",
      " 10  openaccess             20 non-null     object\n",
      " 11  pubmed-id              9 non-null      object\n",
      " 12  affiliations           20 non-null     object\n",
      " 13  authors                20 non-null     object\n",
      "dtypes: object(14)\n",
      "memory usage: 2.3+ KB\n",
      "\n",
      "Abstract results saved to 'scopus_abstract_output_test.csv'\n"
     ]
    }
   ],
   "source": [
    "abstract_test_df = abstract_retrieval_procedures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "796dc84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dc:identifier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dc:title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prism:doi",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prism:coverDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citedby-count",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prism:publicationName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subtypeDescription",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prism:volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prism:issueIdentifier",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "prism:pageRange",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "openaccess",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pubmed-id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "affiliations",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "def55aae-3118-4cc0-aba3-3bf55bdc4a63",
       "rows": [
        [
         "0",
         "SCOPUS_ID:85166028251",
         "Human Digital Twin in the context of Industry 5.0",
         "10.1016/j.rcim.2023.102626",
         "2024-02-01",
         "183",
         "Robotics and Computer-Integrated Manufacturing",
         "Review",
         "85",
         null,
         null,
         "0",
         null,
         "[{'name': 'Benjamin M. Statler College of Engineering and Mineral Resources', 'city': 'Morgantown', 'country': 'United States'}, {'name': 'State Key Laboratory of Fluid Power and Mechatronic Systems', 'city': 'Hangzhou', 'country': 'China'}, {'name': 'School of Mechanical Engineering, Zhejiang University', 'city': 'Hangzhou', 'country': 'China'}, {'name': 'Purdue Polytechnic Institute', 'city': 'West Lafayette', 'country': 'United States'}, {'name': 'The Hong Kong Polytechnic University', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'The Royal Institute of Technology (KTH)', 'city': 'Stockholm', 'country': 'Sweden'}, {'name': 'Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province', 'city': 'Hangzhou', 'country': 'China'}]",
         "[{'name': 'Wang B.', 'affiliation': [{'@id': '60117982', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/60117982'}, {'@id': '60117837', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/60117837'}]}]"
        ],
        [
         "1",
         "SCOPUS_ID:85186172651",
         "Rational molecular and device design enables organic solar cells approaching 20% efficiency",
         "10.1038/s41467-024-46022-3",
         "2024-12-01",
         "152",
         "Nature Communications",
         "Article",
         "15",
         "1",
         null,
         "1",
         "38418862",
         "[{'name': 'Chongqing Institute of Green and Intelligent Technology', 'city': 'Chongqing', 'country': 'China'}, {'name': 'UCLA Samueli School of Engineering', 'city': 'Los Angeles', 'country': 'United States'}, {'name': 'Southern University of Science and Technology', 'city': 'Shenzhen', 'country': 'China'}, {'name': 'Pohang University of Science and Technology', 'city': 'Pohang', 'country': 'South Korea'}, {'name': 'Guangxi University', 'city': 'Nanning', 'country': 'China'}, {'name': 'University of Chinese Academy of Sciences', 'city': 'Beijing', 'country': 'China'}, {'name': 'TaiZhou University', 'city': 'Linhai', 'country': 'China'}, {'name': 'The Hong Kong Polytechnic University', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'Chinese University of Hong Kong', 'city': 'Hong Kong', 'country': 'Hong Kong'}]",
         "[{'name': 'Fu J.', 'affiliation': '60008928'}]"
        ],
        [
         "2",
         "SCOPUS_ID:85188802733",
         "A three-dimensional liquid diode for soft, integrated permeable electronics",
         "10.1038/s41586-024-07161-1",
         "2024-04-04",
         "126",
         "Nature",
         "Article",
         "628",
         "8006",
         "84-92",
         "0",
         "38538792",
         "[{'name': 'City University of Hong Kong', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'The Hong Kong Polytechnic University', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'Southwest Jiaotong University', 'city': 'Chengdu', 'country': 'China'}, {'name': 'Hong Kong Science Park', 'city': 'Hong Kong', 'country': 'Hong Kong'}]",
         "[{'name': 'Zhang B.', 'affiliation': [{'@id': '60013983', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/60013983'}, {'@id': '100819010', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/100819010'}]}]"
        ],
        [
         "3",
         "SCOPUS_ID:85191897734",
         "Emerging contaminants: A One Health perspective",
         "10.1016/j.xinn.2024.100612",
         "2024-07-01",
         "123",
         "Innovation",
         "Review",
         "5",
         "4",
         null,
         "1",
         null,
         "[{'name': 'Hong Kong Baptist University', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'Nanjing University', 'city': 'Nanjing', 'country': 'China'}, {'name': 'Michigan State University', 'city': 'East Lansing', 'country': 'United States'}, {'name': 'University of Alberta', 'city': 'Edmonton', 'country': 'Canada'}, {'name': 'Nanjing Agricultural University', 'city': 'Nanjing', 'country': 'China'}, {'name': 'Chinese Academy of Sciences', 'city': 'Beijing', 'country': 'China'}, {'name': 'Baylor University', 'city': 'Waco', 'country': 'United States'}, {'name': 'Research Center for Eco-Environmental Sciences Chinese Academy of Sciences', 'city': 'Beijing', 'country': 'China'}, {'name': 'The University of Newcastle, Australia', 'city': 'Callaghan', 'country': 'Australia'}, {'name': 'The Hong Kong Polytechnic University', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'College of Agriculture', 'city': 'Cairo', 'country': 'Egypt'}, {'name': 'College of Engineering', 'city': 'East Lansing', 'country': 'United States'}, {'name': 'Marlan and Rosemary Bourns College of Engineering', 'city': 'Riverside', 'country': 'United States'}, {'name': 'Berlin-Brandenburgisches Institut für Biodiversitätsforschung', 'city': 'Berlin', 'country': 'Germany'}, {'name': 'Key Laboratory of Geographic Information Science, Ministry of Education', 'city': 'Shanghai', 'country': 'China'}, {'name': 'INRAE', 'city': 'Paris', 'country': 'France'}, {'name': 'University of Rhode Island Narragansett Bay Campus', 'city': 'Narragansett', 'country': 'United States'}, {'name': 'School of Biological Sciences', 'city': 'Auckland', 'country': 'New Zealand'}, {'name': 'China University of Mining &amp; Technology, Beijing', 'city': 'Beijing', 'country': 'China'}, {'name': 'Ampère', 'city': 'Ecully', 'country': 'France'}, {'name': 'Southern University of Science and Technology', 'city': 'Shenzhen', 'country': 'China'}, {'name': 'Cornell University College of Engineering', 'city': 'Ithaca', 'country': 'United States'}, {'name': 'Chinese Research Academy of Environmental Sciences', 'city': 'Beijing', 'country': 'China'}, {'name': 'Agricultural University of Tirana', 'city': 'Tirana', 'country': 'Albania'}, {'name': 'University of Abomey-Calavi', 'city': 'Cotonou', 'country': 'Benin'}, {'name': 'The University of Queensland', 'city': 'Brisbane', 'country': 'Australia'}, {'name': 'Københavns Universitet', 'city': 'Copenhagen', 'country': 'Denmark'}, {'name': 'Freie Universität Berlin', 'city': 'Berlin', 'country': 'Germany'}, {'name': 'Yunnan University', 'city': 'Kunming', 'country': 'China'}, {'name': 'University of Chinese Academy of Sciences', 'city': 'Beijing', 'country': 'China'}, {'name': 'Northeast Agricultural University', 'city': 'Harbin', 'country': 'China'}, {'name': 'Zhejiang University of Technology', 'city': 'Hangzhou', 'country': 'China'}, {'name': 'Rijksinstituut voor Volksgezondheid en Milieu', 'city': 'Bilthoven', 'country': 'Netherlands'}, {'name': 'Tsinghua University', 'city': 'Beijing', 'country': 'China'}, {'name': 'Shanghai Jiao Tong University', 'city': 'Shanghai', 'country': 'China'}, {'name': 'Universidade de Aveiro', 'city': 'Aveiro', 'country': 'Portugal'}, {'name': 'Université Claude Bernard Lyon 1', 'city': 'Villeurbanne', 'country': 'France'}, {'name': 'Beijing Normal University', 'city': 'Beijing', 'country': 'China'}, {'name': 'Kunming University of Science and Technology', 'city': 'Kunming', 'country': 'China'}, {'name': 'Nanjing Normal University', 'city': 'Nanjing', 'country': 'China'}, {'name': 'Universiteit Leiden', 'city': 'Leiden', 'country': 'Netherlands'}, {'name': 'Technische Universität München', 'city': 'Munich', 'country': 'Germany'}, {'name': 'Monash University', 'city': 'Melbourne', 'country': 'Australia'}, {'name': 'Memorial University of Newfoundland', 'city': \"St John's\", 'country': 'Canada'}, {'name': 'Nankai University', 'city': 'Tianjin', 'country': 'China'}, {'name': 'Hainan University', 'city': 'Haikou', 'country': 'China'}, {'name': 'University of Toronto', 'city': 'Toronto', 'country': 'Canada'}, {'name': 'Universidad de Almería', 'city': 'Almeria', 'country': 'Spain'}, {'name': 'Rheinisch-Westfälische Technische Hochschule Aachen', 'city': 'Aachen', 'country': 'Germany'}, {'name': 'Göteborgs Universitet', 'city': 'Gothenburg', 'country': 'Sweden'}, {'name': 'Peking University', 'city': 'Beijing', 'country': 'China'}, {'name': 'University of Massachusetts Amherst', 'city': 'Amherst', 'country': 'United States'}, {'name': 'University of Waterloo', 'city': 'Waterloo', 'country': 'Canada'}, {'name': 'Fachhochschule Nordwestschweiz FHNW', 'city': 'Windisch', 'country': 'Switzerland'}, {'name': 'Nanjing University of Science and Technology', 'city': 'Nanjing', 'country': 'China'}, {'name': 'Fudan University', 'city': 'Shanghai', 'country': 'China'}, {'name': 'Forschungszentrum Jülich GmbH', 'city': 'Julich', 'country': 'Germany'}, {'name': 'Universität Bonn', 'city': 'Bonn', 'country': 'Germany'}, {'name': 'Guangdong University of Technology', 'city': 'Guangzhou', 'country': 'China'}, {'name': 'Jiangnan University', 'city': 'Wuxi', 'country': 'China'}, {'name': 'The University of Hong Kong', 'city': 'Hong Kong', 'country': 'Hong Kong'}, {'name': 'South China Normal University', 'city': 'Guangzhou', 'country': 'China'}, {'name': 'Cranfield University', 'city': 'Cranfield', 'country': 'United Kingdom'}, {'name': 'Zhejiang University', 'city': 'Hangzhou', 'country': 'China'}, {'name': 'Helsingin Yliopisto', 'city': 'Helsinki', 'country': 'Finland'}, {'name': 'Universitat de València', 'city': 'Valencia', 'country': 'Spain'}, {'name': 'Sino-Danish Center for Education and Research (SDC)', 'city': 'Beijing', 'country': 'China'}]",
         "[{'name': 'Wang F.', 'affiliation': [{'@id': '60019499', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/60019499'}, {'@id': '60027363', '@href': 'https://api.elsevier.com/content/affiliation/affiliation_id/60027363'}]}]"
        ],
        [
         "4",
         "SCOPUS_ID:85171646525",
         "Surface reconstruction and directed electron transport in NiSe2/MoSe2 Mott-Schottky heterojunction catalysts promote urea-assisted water splitting",
         "10.1016/j.apcatb.2023.123312",
         "2024-02-01",
         "113",
         "Applied Catalysis B: Environmental",
         "Article",
         "341",
         null,
         null,
         "0",
         null,
         "[{'name': 'Guangdong University of Technology', 'city': 'Guangzhou', 'country': 'China'}, {'name': 'The Hong Kong Polytechnic University', 'city': 'Hong Kong', 'country': 'Hong Kong'}]",
         "[{'name': 'Xu X.', 'affiliation': '60007155'}]"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dc:identifier</th>\n",
       "      <th>dc:title</th>\n",
       "      <th>prism:doi</th>\n",
       "      <th>prism:coverDate</th>\n",
       "      <th>citedby-count</th>\n",
       "      <th>prism:publicationName</th>\n",
       "      <th>subtypeDescription</th>\n",
       "      <th>prism:volume</th>\n",
       "      <th>prism:issueIdentifier</th>\n",
       "      <th>prism:pageRange</th>\n",
       "      <th>openaccess</th>\n",
       "      <th>pubmed-id</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCOPUS_ID:85166028251</td>\n",
       "      <td>Human Digital Twin in the context of Industry 5.0</td>\n",
       "      <td>10.1016/j.rcim.2023.102626</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>183</td>\n",
       "      <td>Robotics and Computer-Integrated Manufacturing</td>\n",
       "      <td>Review</td>\n",
       "      <td>85</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'name': 'Benjamin M. Statler College of Engi...</td>\n",
       "      <td>[{'name': 'Wang B.', 'affiliation': [{'@id': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCOPUS_ID:85186172651</td>\n",
       "      <td>Rational molecular and device design enables o...</td>\n",
       "      <td>10.1038/s41467-024-46022-3</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>152</td>\n",
       "      <td>Nature Communications</td>\n",
       "      <td>Article</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>38418862</td>\n",
       "      <td>[{'name': 'Chongqing Institute of Green and In...</td>\n",
       "      <td>[{'name': 'Fu J.', 'affiliation': '60008928'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCOPUS_ID:85188802733</td>\n",
       "      <td>A three-dimensional liquid diode for soft, int...</td>\n",
       "      <td>10.1038/s41586-024-07161-1</td>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>126</td>\n",
       "      <td>Nature</td>\n",
       "      <td>Article</td>\n",
       "      <td>628</td>\n",
       "      <td>8006</td>\n",
       "      <td>84-92</td>\n",
       "      <td>0</td>\n",
       "      <td>38538792</td>\n",
       "      <td>[{'name': 'City University of Hong Kong', 'cit...</td>\n",
       "      <td>[{'name': 'Zhang B.', 'affiliation': [{'@id': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCOPUS_ID:85191897734</td>\n",
       "      <td>Emerging contaminants: A One Health perspective</td>\n",
       "      <td>10.1016/j.xinn.2024.100612</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>123</td>\n",
       "      <td>Innovation</td>\n",
       "      <td>Review</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'name': 'Hong Kong Baptist University', 'cit...</td>\n",
       "      <td>[{'name': 'Wang F.', 'affiliation': [{'@id': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCOPUS_ID:85171646525</td>\n",
       "      <td>Surface reconstruction and directed electron t...</td>\n",
       "      <td>10.1016/j.apcatb.2023.123312</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>113</td>\n",
       "      <td>Applied Catalysis B: Environmental</td>\n",
       "      <td>Article</td>\n",
       "      <td>341</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'name': 'Guangdong University of Technology'...</td>\n",
       "      <td>[{'name': 'Xu X.', 'affiliation': '60007155'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dc:identifier                                           dc:title  \\\n",
       "0  SCOPUS_ID:85166028251  Human Digital Twin in the context of Industry 5.0   \n",
       "1  SCOPUS_ID:85186172651  Rational molecular and device design enables o...   \n",
       "2  SCOPUS_ID:85188802733  A three-dimensional liquid diode for soft, int...   \n",
       "3  SCOPUS_ID:85191897734    Emerging contaminants: A One Health perspective   \n",
       "4  SCOPUS_ID:85171646525  Surface reconstruction and directed electron t...   \n",
       "\n",
       "                      prism:doi prism:coverDate citedby-count  \\\n",
       "0    10.1016/j.rcim.2023.102626      2024-02-01           183   \n",
       "1    10.1038/s41467-024-46022-3      2024-12-01           152   \n",
       "2    10.1038/s41586-024-07161-1      2024-04-04           126   \n",
       "3    10.1016/j.xinn.2024.100612      2024-07-01           123   \n",
       "4  10.1016/j.apcatb.2023.123312      2024-02-01           113   \n",
       "\n",
       "                            prism:publicationName subtypeDescription  \\\n",
       "0  Robotics and Computer-Integrated Manufacturing             Review   \n",
       "1                           Nature Communications            Article   \n",
       "2                                          Nature            Article   \n",
       "3                                      Innovation             Review   \n",
       "4              Applied Catalysis B: Environmental            Article   \n",
       "\n",
       "  prism:volume prism:issueIdentifier prism:pageRange openaccess pubmed-id  \\\n",
       "0           85                  None            None          0      None   \n",
       "1           15                     1            None          1  38418862   \n",
       "2          628                  8006           84-92          0  38538792   \n",
       "3            5                     4            None          1      None   \n",
       "4          341                  None            None          0      None   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  [{'name': 'Benjamin M. Statler College of Engi...   \n",
       "1  [{'name': 'Chongqing Institute of Green and In...   \n",
       "2  [{'name': 'City University of Hong Kong', 'cit...   \n",
       "3  [{'name': 'Hong Kong Baptist University', 'cit...   \n",
       "4  [{'name': 'Guangdong University of Technology'...   \n",
       "\n",
       "                                             authors  \n",
       "0  [{'name': 'Wang B.', 'affiliation': [{'@id': '...  \n",
       "1     [{'name': 'Fu J.', 'affiliation': '60008928'}]  \n",
       "2  [{'name': 'Zhang B.', 'affiliation': [{'@id': ...  \n",
       "3  [{'name': 'Wang F.', 'affiliation': [{'@id': '...  \n",
       "4     [{'name': 'Xu X.', 'affiliation': '60007155'}]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0105ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_retrieval_procedures():\n",
    "    try:\n",
    "        scopus_credentials, db_credentials = get_credentials()\n",
    "\n",
    "        # Load the CSV file with DOIs\n",
    "        csv_file = 'polyu_research_output.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            print(f\"Loaded {len(df)} records from {csv_file}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\n",
    "                f\"No file found at {csv_file}. Please run scopus_research_procedures first.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file: {e}\")\n",
    "            return\n",
    "\n",
    "        # Get unique DOIs\n",
    "        dois = df['prism_doi'].dropna().unique()\n",
    "        print(f\"Found {len(dois)} unique DOIs to process\")\n",
    "\n",
    "        abstract_results = []\n",
    "        for doi in dois:\n",
    "            try:\n",
    "                abstract_data = abstract_retrieval(scopus_credentials, doi)\n",
    "                processed_abstract = process_abstract_retrieval_results(\n",
    "                    abstract_data)\n",
    "                abstract_results.append(processed_abstract)\n",
    "                print(\n",
    "                    f\"Successfully retrieved and processed abstract for DOI: {doi}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing abstract for DOI {doi}: {e}\")\n",
    "\n",
    "        if not abstract_results:\n",
    "            print(\"No abstract results to process.\")\n",
    "            return\n",
    "\n",
    "        # Convert abstract results to DataFrame\n",
    "        abstract_df = pd.DataFrame(abstract_results)\n",
    "\n",
    "        # Print the first few rows of the abstract DataFrame\n",
    "        print(\"\\nFirst few rows of the abstract DataFrame:\")\n",
    "        print(abstract_df.head())\n",
    "\n",
    "        # Print DataFrame info\n",
    "        print(\"\\nAbstract DataFrame info:\")\n",
    "        abstract_df.info()\n",
    "\n",
    "        # Save abstract results to CSV\n",
    "        abstract_csv_file = 'scopus_abstract_output.csv'\n",
    "        abstract_df.to_csv(abstract_csv_file, index=False)\n",
    "        print(f\"\\nAbstract results saved to '{abstract_csv_file}'\")\n",
    "        \n",
    "        return abstract_df\n",
    "\n",
    "        # Upload abstract results to database\n",
    "        # try:\n",
    "        #     abstract_retrieval_data_uploader(\n",
    "        #         db_credentials, abstract_df, table_name='scopus_abstract_output')\n",
    "        #     print(\"Abstract Retrieval Data has been uploaded to DB.\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error uploading abstract data to database: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f070f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd1dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
